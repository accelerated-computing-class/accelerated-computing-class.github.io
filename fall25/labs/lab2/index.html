<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>6.S894</title><base href="/fall25/labs/lab2/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall25/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall25/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall25/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall25/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/fall25/assets/main.css" rel="stylesheet"><link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet"><link href="/fall25/assets/favicon.png" rel="icon" type="image/png"></head><body><header><nav><h1><a href="/fall25/">6.S894</a></h1>
<p><a href="/fall25/calendar">Calendar</a></p>
<p><a href="/fall25/labs">Labs</a></p>
<p><a href="/fall25/lectures">Lectures</a></p>
<p><a href="/fall25/syllabus">Syllabus</a></p>
<p><a href="/fall25/resources">Resources</a></p>
<p><a href="/fall25/contact">Contact</a></p>
<p><a href="/fall25/piazza">Piazza</a></p>
</nav></header><main><h1>Lab 2: Massively-Parallel Mandelbrot</h1>
<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint</strong>: Due Monday, September 15, 11:59pm. On the Gradescope assignment for the Lab 2 checkpoint, submit your answers to the prelab questions (Part 0 of the lab).</p>
</li>
<li>
<p><strong>Final Submission</strong>: Due Saturday, September 20, 11:59pm. On the Gradescope assignments for Lab 2, submit your <strong>completed code</strong> for <code>mandelbrot_cpu_2.cpp</code> and <code>mandelbrot_gpu_2.cpp</code>, as well as a PDF writeup containing your answers to Questions 1 - 8 below, answering each question with roughly a sentence or two.</p>
</li>
</ul>
<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab2">lab repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab2.git</span>
</span></pre>
<h3>Changes to Starter Code vs Lab 1</h3>
<p>For Lab 2, you should be aware that we’ve made a few changes to the reference implementation of the Mandelbrot algorithm in the starter code, relative to what was in the starter code for Lab 1.</p>
<p>The line <code>float y = w - x2 - y2 + cy;</code> is now <code>float y = w - (x2 + y2) + cy;</code>. This may seem like a trivial change, but it speeds up the algorithm by allowing the compiler to reuse the expression <code>x2 + y2</code> computed in the loop condition, and also slightly alters the algorithm’s output due to the (maddening) non-associativity of floating-point math.</p>
<p>Additionally, we’ve adjusted the parameters of the Mandelbrot image we’re generating to be more ambitious. The image is now:</p>
<ul>
<li>
<p><strong>Higher resolution.</strong></p>
</li>
<li>
<p><strong>Higher quality.</strong> (More iterations per pixel)</p>
</li>
<li>
<p><strong>Zoomed in on a more interesting part of the Mandelbrot set.</strong> <br> (Check out the images in <code>./telerun-out</code> to see what it looks like!)</p>
</li>
</ul>
<p>To control what part of the Mandelbrot set we’re zoomed in to, we’ve added three new constant definitions in the starter code: <code>window_zoom</code>, <code>window_x</code>, and <code>window_y</code>. If you copy any code you wrote for Lab 1 for use in this lab, you’ll need to refer to these new constants instead of using the hard-coded window parameters we used in Lab 1.</p>
<h2>Introduction</h2>
<h3>Goals for This Lab</h3>
<p>In <a href="/fall25/labs/lab1">Lab 1</a>, we looked at ways of using <em>vector parallelism</em> to accelerate an algorithm for rendering the Mandelbrot fractal. We also promised that later we would look at ways to add more levels of parallelism. The time has come!</p>
<p>In this lab, we’ll be starting from our vector-parallel Mandelbrot implementation and adding three new kinds of parallelism:</p>
<ol>
<li>
<p><strong>Instruction-Level Parallelism:</strong> parallelizing execution of different instructions within a single instruction stream.</p>
</li>
<li>
<p><strong>Multi-Core Parallelism:</strong> parallelizing across multiple physical cores, each running its own instruction stream.</p>
</li>
<li>
<p><strong>Multi-Threaded Parallelism:</strong> parallelizing execution of multiple instruction streams within a single physical core.</p>
</li>
</ol>
<p>Just like we did in Lab 1, throughout this lab, we’ll be focusing on the <strong>correspondences</strong> between the different levels of parallelism which exist in CPUs and GPUs. As we’ll see, the analogies between CPU and GPU hardware remain quite close even as we scale up our programs to start exploiting instruction-level, multi-core, and multi-threaded parallelism.</p>
<p>In this lab, as we write programs which are able to use the GPU to its full potential, we’ll also finally start to see our GPU programs overtaking our CPU programs in performance.</p>
<h3>A Note on Terminology: What is a “Thread?” (And What is a “Warp?”)</h3>
<p>Before we get into the details of the lab, we want to clarify some terminology which can become overloaded and confusing when talking about GPUs.</p>

<p>As you might have guessed from the name of the special variable <code>threadIdx.x</code> in Lab 1, NVIDIA uses the word “thread” to refer to <em>a single instance of the program defined by your kernel</em>. But because the GPU executes all instructions in a SIMD fashion across 32 instances of your program at a time, what NVIDIA refers to as a “thread” is much closer to what we’d refer to as a <strong>vector lane</strong> on a CPU.</p>
<p>By contrast, NVIDIA uses the term “<a href="https://en.wikipedia.org/wiki/Warp_and_weft">warp</a>” to refer to what we’d traditionally call a “thread”: a single stream of instructions which logically executes sequentially, and that instruction stream’s associated state (registers, etc.). Note that because all instructions executed on an NVIDIA GPU are executed in 32-wide SIMD fashion, a “warp” consists of 32 of the things-NVIDIA-calls-“threads”.</p>
<p>For the purposes of this course, we will try to explicitly use the term “<strong>CUDA thread</strong>” when referring to the things NVIDIA calls threads, and we encourage you to do the same during live lab, in write-ups, on Piazza, etc.</p>
<p>When we talk about “threads” generically — as in the phrase “multi-threading” — we’re referring to the thing that NVIDIA calls “warps.”</p>
<p>We can summarize the situation in the following table of correspondences:</p>
<div class="table-container"><table><thead><tr><th>GPU Concept</th><th>CPU Concept</th></tr></thead><tbody>
<tr><td>CUDA Thread</td><td>Vector Lane</td></tr>
<tr><td>Warp</td><td>Thread</td></tr>
</tbody></table>
</div>
<h3>Background: A Closer Look at the Hardware</h3>
<p>Before we start extending our code, it will be useful to have a clear mental picture of what hardware resources are available on the machines we’re programming.</p>

<h4>The CPU</h4>
<p>The CPU we’re using for this lab is an <a href="https://www.techpowerup.com/cpu-specs/ryzen-7-7700.c2960">AMD Ryzen 7 7700</a>. This CPU is clocked at 3.8 GHz and consists of <strong>8 <em>cores</em></strong>, each of which can execute its own independent stream of scalar and vector instructions. The scalar and vector programs we wrote in Lab 1 made use of only a <strong>single</strong> one of these cores, and ignored the other 7.</p>
<p>Each core has its own <strong>register file</strong> for storing thread state, its own <strong>control logic</strong>, and its own <strong>functional units</strong> like ALUs and FPUs. Although it’s not too relevant for the compute-dominated workload we’re studying in this lab, each core also has its own <strong>L1 cache</strong>.</p>
<p>Each core on this CPU supports <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading">simultaneous multi-threading (SMT)</a> with <strong>at most 2 concurrent <em>threads</em></strong> of execution, with each thread having its own program counter and register state. When different instructions from these threads require the use of different hardware resources, they are able to execute in parallel; when they contend for the same shared hardware resources, they are forced to execute in sequence.</p>
<p>We show a schematic view of the CPU below:</p>
<p><img src="images/cpu_schematic.svg" alt="CPU schematic" /></p>
<h4>The GPU</h4>
<p>The GPU we’re using for this lab is an <a href="https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171">NVIDIA RTX 4000 Ada</a>, clocked at 2.175 GHz. Unlike the CPU, which consists of a flat array of cores, the processing elements on our GPU are organized into a two-level hierarchy. In NVIDIA’s terminology:</p>
<ol>
<li>
<p>The GPU consists of <strong>48 <em>streaming multiprocessors</em> (SMs)</strong>.</p>
</li>
<li>
<p>Each SM consists of <strong>4 <em>warp schedulers</em></strong>. <sup class="footnote-reference"><a href="#1">1</a></sup></p>
</li>
</ol>
<p>For our purposes, we can regard each individual warp scheduler as <strong>analogous to a CPU core</strong>. Just like a CPU core, each warp scheduler can execute its own independent stream of vector instructions, and has its own register file, control logic, and functional units. Each cycle, the 4 warp schedulers can each issue an instruction for their respective warps, provided they are not waiting for an instruction to finish. In this lab, we’ll use the word “core” generically to refer to both CPU cores and to warp schedulers.</p>
<p>If a warp scheduler is analogous to a CPU core, you may be wondering about the significance of the “SM” level of the hierarchy – why not just think of the GPU as a flat array of 192 warp schedulers? The answer lies in the memory system: the <strong>L1 cache</strong>, as well as some other memory resources, are <strong>shared at the level of an SM</strong>. The fact that the L1 cache is shared won’t be directly relevant for this lab, but it may be helpful for understanding why the concept of an “SM” exists at all.</p>
<p>Similar to the cores on our CPU, each warp scheduler can manage multiple concurrent warps at a time. Specifically, each warp scheduler on our GPU supports <a href="https://en.wikipedia.org/wiki/Temporal_multithreading">fine-grained temporal multi-threading</a> between <strong>at most 12 concurrent warps</strong>. The warp scheduler can issue an instruction for at most one warp per cycle, and multi-cycle instructions from different warps can execute in parallel with each other as long as they don’t contend for the same shared hardware resources.</p>
<p>Just like on the CPU, the scalar and vector GPU programs we wrote in Lab 1 made use of only a <strong>single</strong> warp scheduler, and ignored the other 191 (!) warp schedulers on the machine.</p>
<p>We show a schematic view of the GPU below:</p>
<!-- * **Clock frequency:** 1.6 GHz
* **Number of warp schedulers:** 192
* **Max concurrent warps per warp scheduler:** 12 -->

<p><img src="images/gpu_schematic.svg" alt="GPU schematic" /></p>
<h2>Part 0 (Prelab): Instruction Latency of an FMA</h2>
<p>In Lab 1, several of the Mandelbrot implementations made use of the <em>fused multiply-add</em> (FMA) instruction. Unlike performing a multiplication and an addition as two separate instructions, an FMA combines them into one: it multiplies one operand by another and then adds a third operand to the result, all in a single step. This not only reduces the total number of instructions executed by the program, but can also improve numerical precision.</p>
<p>In this part, we will empirically study the instruction latency of a floating-point FMA on the GPU and explore how this latency influences the decisions made by the warp scheduler.</p>
<blockquote>
<p><strong>Deliverable:</strong> The GPU provides an <code>FFMA</code> instruction to perform fused multiply-add operations on floating-point values.
To measure the latency of the <code>FFMA</code> instruction, write code in the <code>measure_fma_latency(..)</code> that produces an <code>FFMA</code> instruction during compilation.</p>
</blockquote>
<!-- ```C++
// Measure latency of a single FMA by reading the GPU clock.
asm volatile("mov.u64 %0, %%clock64;" : "=l"(start_time));

/// <- your code here -> 
(void) x;

asm volatile("mov.u64 %0, %%clock64;" : "=l"(end_time));
``` -->
<p>You can use the <code>-s</code> flag with telerun to verify your code generates an <code>FFMA</code> instruction in the SASS output. Here’s an example <code>FFMA</code> instruction:</p>
<pre><code>/*0050*/                   FFMA.FTZ R0, R0, R0, R0 ;     /* 0x0000000000007223 */
                                                         /* 0x004fc80000010000 */
</code></pre>
<p>Running <code>fma_latency.cu</code> should print the latency of the instruction <code>FFMA</code> in cycles.</p>
<pre><code>    Latency of measure_fma_latency code snippet = &lt;REPORTED&gt; cycles 
</code></pre>
<p><strong>Helpful Tip:</strong>  To make the timing more stable, chain dependent FMA instructions so the compiler cannot reorder them. Finally, divide the total execution time by the number of operations to compute the average latency per instruction.</p>
<blockquote>
<p><strong>Prelab Question 1:</strong> What is the latency of an <code>FFMA</code> instruction?</p>
</blockquote>
<h2>Instruction-Level Parallelism</h2>
<p>In practice, even though complex instructions like <code>FFMA</code> take multiple cycles to finish, modern processors do not stall waiting for each instruction to complete. Instead, they exploit  <em><a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">instruction-level parallelism</a></em> (ILP), which refers to cases where a processor exploits independence between instructions <strong>within one instruction stream</strong> in order to run them in parallel on different hardware resources.</p>
<p>For example, using ILP, we can exploit <strong>more parallelism</strong> in our vector Mandelbrot program <strong>using just the single thread that we already have.</strong></p>
<p>The exact microarchitectural details of when and how a core can exploit ILP differ between our CPU and GPU <sup class="footnote-reference"><a href="#2">2</a></sup>, but the implications for software performance engineering are similar on both platforms. In general, sections of code with <strong>sequential dependencies</strong> like the following will not be able to exploit ILP:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> No ILP:
</span><span class="highlight-storage highlight-type highlight-c">float</span> a0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-comment highlight-block highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">/*</span> ... <span class="highlight-punctuation highlight-definition highlight-comment highlight-c">*/</span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a0 <span class="highlight-keyword highlight-operator highlight-c">*</span> a0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a1 <span class="highlight-keyword highlight-operator highlight-c">*</span> a1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a3 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a2 <span class="highlight-keyword highlight-operator highlight-c">*</span> a2<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>whereas code containing <strong>independent instructions</strong> will provide the processor with opportunities to exploit ILP:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> Opportunities for ILP:
</span><span class="highlight-storage highlight-type highlight-c">float</span> b0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c0 <span class="highlight-keyword highlight-operator highlight-c">*</span> c0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c1 <span class="highlight-keyword highlight-operator highlight-c">*</span> c1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c2 <span class="highlight-keyword highlight-operator highlight-c">*</span> c2<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b3 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c3 <span class="highlight-keyword highlight-operator highlight-c">*</span> c3<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>Depending on the latency and throughput with which the processor can execute a given type of instruction, running multiple instructions in parallel is often necessary in order to drive the hardware at its peak throughput.</p>
<p>Let’s look how instruction-level parallelism plays out on our GPU.</p>
<blockquote>
<p><strong>Prelab Question 2:</strong>  In <code>fma-latency.cu</code>, complete the <code>fma_latency_interleaved</code> function using a sequence of explicitly interleaved FMA operations designed to exploit ILP. What latency do you observe while running this sequence of operations? Does the observed number contradict your observations from before?</p>
</blockquote>
<blockquote>
<p><strong>Prelab Question 3:</strong>  While <code>fma_latency_interleaved</code> explicitly interleaves the instructions, write an analogous version in <code>fma_latency_no_interleaved</code> that executes the instructions without explicitly interleaving them (the program should <em>still</em> have ILP, but the source code should not explicitly expose it). Does <code>fma_latency_no_interleaved</code> run with the same latency as <code>fma_latency_interleave</code>?  Why, or why not?</p>
</blockquote>
<h2>Using ILP in a Simple Program</h2>
<p>Now that we understand the latency of an <code>FFMA</code> instruction, let’s put it to use in a simple program. The file <code>warp_scheduler.cu</code> contains a simple GPU kernel that processes an input array by performing a sequence of FMAs on each element and storing the results in an output array.</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> Begin computing from the start of the tile allocated to the thread.
</span><span class="highlight-keyword highlight-control highlight-c++">for</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span><span class="highlight-storage highlight-type highlight-c">int</span> i <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-constant highlight-numeric highlight-c++">0</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> i <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> Tile<span class="highlight-punctuation highlight-terminator highlight-c++">;</span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span>i<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> tmp <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> in<span class="highlight-meta highlight-brackets highlight-c++"><span class="highlight-punctuation highlight-section highlight-brackets highlight-begin highlight-c++">[</span>index <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> i<span class="highlight-punctuation highlight-section highlight-brackets highlight-end highlight-c++">]</span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> Repeat the FMA operation NumOps times
</span><span class="highlight-meta highlight-preprocessor highlight-c++"><span class="highlight-keyword highlight-control highlight-import highlight-c++">#pragma</span> unroll
</span>    <span class="highlight-keyword highlight-control highlight-c++">for</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span><span class="highlight-storage highlight-type highlight-c">int</span> op <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-constant highlight-numeric highlight-c++">0</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> op <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> NumOps<span class="highlight-punctuation highlight-terminator highlight-c++">;</span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span>op<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
        tmp <span class="highlight-keyword highlight-operator highlight-assignment highlight-augmented highlight-c">+=</span> tmp <span class="highlight-keyword highlight-operator highlight-c">*</span> <span class="highlight-constant highlight-numeric highlight-c++">3.0f</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> <span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> stage 1 work
</span>    <span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
    out<span class="highlight-meta highlight-brackets highlight-c++"><span class="highlight-punctuation highlight-section highlight-brackets highlight-begin highlight-c++">[</span>index <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> i<span class="highlight-punctuation highlight-section highlight-brackets highlight-end highlight-c++">]</span></span> <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> tmp<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
</span></pre>
<p>For this experiment, let’s express the launch parameters in terms of the total number of warps (or, what we call “threads”) used in the computation. Since each warp consists of 32 lanes, the total number of warps can be calculated with a simple formula:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext># Warps</mtext><mo>=</mo><mfrac><mrow><mtext># Blocks</mtext><mo>×</mo><mtext># Lanes per Block</mtext></mrow><mn>32</mn></mfrac></mrow><annotation encoding="application/x-tex">
\text{\# Warps} = \frac{\text{\# Blocks} \times \text{\# Lanes per Block}}{32}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord"># Warps</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">32</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord"># Blocks</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord"># Lanes per Block</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>The starter code provided in <code>warp_scheduler.cu</code> runs our kernel under different configurations to see how much of our machine’s peak throughput it achieves. In particular, across different runs, <code>warp_scheduler.cu</code> will launch the kernel while varying the numbers of warps. Importantly, the amount of work performed by each warp and each lane inside the warp will remain <em>exactly</em> the same across these runs. Running the file should produce output as follows:</p>
<pre><code>    # Warps,    # FLOPS
    96,         3.48436e+12
    192,        6.92671e+12
    288,        1.04813e+13
    384,        1.39468e+13
    480,        1.70766e+13
    576,        2.05939e+13
    672,        2.39468e+13
    768,        2.64575e+13
    864,        1.59401e+13
    960,        1.77052e+13
</code></pre>
<p>To plot this data, we provide a minimal script that you can run using the following command:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">python3</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> plot.py <span class="highlight-keyword highlight-operator highlight-assignment highlight-redirection highlight-shell">&lt;</span> <span class="highlight-keyword highlight-operator highlight-assignment highlight-redirection highlight-shell">&lt;</span>telerun-out<span class="highlight-keyword highlight-operator highlight-assignment highlight-redirection highlight-shell">&gt;</span>/<span class="highlight-keyword highlight-operator highlight-assignment highlight-redirection highlight-shell">&lt;</span>job-id<span class="highlight-keyword highlight-operator highlight-assignment highlight-redirection highlight-shell">&gt;</span>/execute-log.txt</span>
</span></pre>
<p>A curious pattern emerges in the data: even though the work/warp and work/lane remains constant across each run, launches with different numbers of warps achieve different fractions of the peak throughput.</p>
<p><img src="images/fma_achieved_latency.svg" alt="FMA Latency" /></p>
<blockquote>
<p><strong>Prelab Question 4:</strong> When do you observe the benchmark hitting maximum throughput? Considering the latency of an FMA instruction, what may explain this behavior?
<em>Remember: Our GPU has a total number of 48 * 4 warp schedulers (48 SM and 4 warp scheduler/SM).</em></p>
</blockquote>


<h2>Part 1: ILP in Mandelbrot</h2>
<h3>Implementation: Increasing ILP on the CPU</h3>
<p>Revisiting our Mandelbrot program, we can analyze its inner loop to see what kind of opportunities for ILP it exposes to the processor:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-keyword highlight-control highlight-c++">while</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span>x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y2 <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;=</span> <span class="highlight-constant highlight-numeric highlight-c++">4.0f</span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">&amp;&amp;</span> iters <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> max_iters<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> x <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">-</span> y2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> cx<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> y <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> w <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">-</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span>x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y2<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> cy<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    x2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x <span class="highlight-keyword highlight-operator highlight-c">*</span> x<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    y2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> y <span class="highlight-keyword highlight-operator highlight-c">*</span> y<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> z <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    w <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> z <span class="highlight-keyword highlight-operator highlight-c">*</span> z<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span>iters<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
</span></pre>
<p>We can see that our Mandelbrot program <em>already</em> provides the processor with some opportunities to exploit ILP: for example, the instructions <code>x2 = x * x;</code>, <code>y2 = y * y;</code>, and <code>z = x + y;</code> are independent. On the other hand, there are also some sequential bottlenecks: for example, the instruction <code>w = z * z;</code> depends directly on the previous instruction <code>z = x + y;</code>.</p>
<p>Using the GPU version of our program as an example, we can visualize the dependency relationships between the different <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIAruiakl9ZATwDKjdAGFUtEywYgATKUcAZPAZMADl3ACNMYhAAZgBWUgAHVAVCOwYXNw9vROTUgQCg0JYIqLjLTGtbASECJmICDPdPHytMGzSauoICkPDImPiFWvrGrJbh7sDe4v64gEpLVBNiZHYOAFIvaMDkNywAanXop2QWJgIEI%2Bx1jQBBTe2GXbNMQ%2BPkIfxBK5v7u7QGENMKoEsR9lRaKhzvsAO6BdCoGEAfQAXqhUCw3gARfZcAB0Gn2AHpcRoyRoCVQjgAhX4AoEgsEQqEEWHwxFI1TY/YAWgpAHZJNEAGxCgAcYv5vP2FNi%2BwAVGyGAjkWiMTS6QIGaDwZDoXDlRyAJ7cilcaJcMVeWIATg0kmlsoVSpVqPRLA1d1%2BRJJTmUcn2IjEdX2AFlDFhaGFiKhWUpWcBGJFzmk8d6SdSmEp0PsBPsLq9Nl5UAlbCw8CjMDnMApRAlXmXXmJgCRCAgPV4vPtAun9ggCAQEgoQD7GHi4QBrPD1/BMPEkYBEyd4InKSEDwLAJHN1sXFgKJH8YhIgtI8PKirR2NI%2BO/JFI4CQsJie/7ABuqDwObOF6jMYID4JCYN6iAYxAQCYgQENEXgnt2LBbiklakPskGCDBcFnKoSKEJECgoWh0Gway8pLAQcyHPytJ3PstHgiQ%2BwQVBGGsngpo0t2bxOPBiEVpgHGbLSXjUngFHrFRvx0VJ9Fgkx6HEfs2jsdE1KKVxPE3nxAnCYJ2hiRJNHSUZzLQsgXJHDi1B6gQEB6cSuoshAeAIZplZzBRioGq6aqYoJLocqonq3EZxnWfsyAmhZjEmTZon2TFTkuUhmDuc6Xkcj5hzCf5yJGkFkkhVJMX7KoXZRRSGhUipBWFbRxVGmV0Q4hVVXUcFtV1WFMKmpSQUdbRhEsd2BB4cpbX9bCCB0K8EClVlqkNVxUWSJSWXCpswrDaNRzcVhOEjcQCj6eNE10cV5lNSVXY8vsi1%2BWZfWnZ1LK3dy3U3bNZXZQ1YnZRFj1PXNUVcoqgXVYZT13ZdJqKnl4PtU9xUoty5nfQDp3dVFyOKii6MTYJgm4YdeMheJWK/DVtU%2BvsADqxC4fsxA1iYtAEGmEO1WR6yxCJzrObxlbzYp3M4lFRMKCTtFk5T0tevyWIcAstCcLEvCeNwvCoJwThyFiwVOAT2UKEsKyFlsPCkAQmiK6QEBIGgLAJNNZAUBADtO/QUQMG%2ByDIFwXiSCYXB8HQB3DhAYTW6QYSBHURqcBbMfMMQRoAPJhNobRWxrpAO2wgipwwtDxxwWikFgYQmMAThiLQw451gP6PmsZf4Ez7RvjWUfAm0JgjQnvBQRUUe0Hg0Zxy4WBRwQ9MsAPpCd8QYTJJgWKYE3o9GNbCwQkwwAKAAangmAwqn9bqxb/CCEG4hSDIgiKCo6ilzoegGFvpjmPoY/DpACwllUQEnAeSp2iLyHkJglCHizABM4FxeSJieBgJsKxLhNXdn3TASIxTClzsgiyCgWDYOFJrRe9MsC/wgAsVo7R7AQEcKMTwwd/BTCKCUPQSQUiAMYRw3IgCehsP6MHGhgDOgjFcE0PQIiOgTAEX0KIwiJg8MUV0ORMwFHUJNqsCQSsVZqyjlrDg%2BxggHycNxf2eIVpcEYrgQgDEHhcDmLwbOWh3KkAnCAWIGh9CcEkPol%2BpBDG8GHN4lxNs4CwCQFgN8eBVjkEoO7Z2wRWBrBMWY3EXhLF4gttE2J7BlAABUAAavh8BEHIXoK%2BwhQLsDvlUx%2Bago66GERULOaQHDKmUb4ZUaj2HB04XkdIEisj9L4WkXpQjyiVBkV0Lp0jqiyNYfIqRSjhlMMsIswoyzHGWyZpgL8IANC6I4KrUg6sy6GLXjE1Y%2Bwj4n0iMY0x5jMlWJsWU%2Bx5sUIuEds7LKFonGW23gsBAmAmBYCiFQ9xnjvHKw4H4s5BjODBMOYCl%2BbjYVeH8RcpFqLXELEXikewkggA%3D">PTX assembly</a> instructions in the inner loop as follows:</p>
<p><img src="images/ilp_graph.svg" alt="Instruction dependency graph" /></p>
<p>(If that’s hard to read, you can view the full-size image <a href="images/ilp_graph.svg">here</a>.)</p>
<p>Even though this sequence of instructions contains some amount of ILP, that ILP is not sufficient to fully utilize the hardware of one of our GPU’s warp schedulers: in particular, there is no point in the inner loop where the processor has the opportunity to run enough independent floating-point arithmetic instructions simultaneously. Similar considerations apply to our CPU version.</p>
<p>At first, it might seem like there isn’t much we can do to increase the amount of ILP in our program. It looks like some of the instructions in our algorithm just fundamentally need the results of earlier instructions to be available before they can execute.</p>
<p>If we only cared about computing the value of a single pixel, or a single vector of pixels, it would indeed be difficult to increase the amount of ILP in our program. But we don’t want to compute just a single vector of pixels – we want to compute <strong>thousands</strong> of vectors of pixels! Even if the instructions required to compute each individual vector of pixels are inherently sequential, the instructions required to compute <em>multiple different vectors</em> of pixels are independent of each other. We can exploit this observation to significantly increase ILP.</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_ilp</code> using an algorithm employing both vectorization (from Lab 1) and techniques to increase ILP. You can increase ILP by reorganizing your computation so that <strong>each</strong> inner loop iteration processes <strong>multiple independent vectors’ worth of pixels</strong> at a time.</p>
</blockquote>
<blockquote>
<p><strong>Deliverable:</strong> Do the same for the GPU, by editing the file <code>mandelbrot_gpu_2.cu</code> to implement the functions <code>mandelbrot_gpu_vector_ilp</code> and <code>launch_mandelbrot_gpu_vector_ilp</code>. Keep the launch configuration of your kernel at <code>&lt;&lt;&lt;1, 32&gt;&gt;&gt;</code>, like we did in Lab 1; this is directly analogous to how we’re running a single thread of vector instructions on the CPU.</p>
</blockquote>
<p>Here are some things you might want to consider in your CPU and GPU implementations:</p>
<ol>
<li>
<p>What should happen to the <strong>state variables</strong> in your program if you’re processing multiple vectors’ worth of pixels at a time? For example, how should you deal with the variables <code>x2</code> and <code>y2</code>?</p>
</li>
<li>
<p>How should you handle <strong>control flow</strong>? How do you deal with differences between the number of inner loop iterations required to compute different vectors of pixels?</p>
</li>
<li>
<p>Do you think certain <strong>control flow patterns</strong> might impose <strong>different costs and benefits</strong> on the <strong>CPU vs the GPU</strong>? What are some possible control flow strategies you might try on each platform?</p>
</li>
<li>
<p><strong>How many</strong> different vectors of pixels should you work on at a time? Can you set up a system that lets you easily test the effects of trying different values for that parameter?</p>
</li>
<li>
<p><strong>Where in the image</strong> do you want to take your multiple vectors of pixels from? Do you want to process one long row at a time? Or 2D rectangular tiles? Something more complicated?</p>
</li>
</ol>
<p>Additionally, when you write your code, you may find it helpful to not that the the compiler supports an annotation called <strong><code>#pragma unroll</code></strong> (<a href="https://releases.llvm.org/4.0.0/tools/clang/docs/AttributeReference.html#pragma-unroll-pragma-nounroll">link</a>) which can be used to replace an <code>n</code>-iteration <code>for</code> loop with <code>n</code> copies of its body. For example, if you write: <br></p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-meta highlight-preprocessor highlight-c++"><span class="highlight-keyword highlight-control highlight-import highlight-c++">#pragma</span> unroll
</span><span class="highlight-keyword highlight-control highlight-c++">for</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span><span class="highlight-support highlight-type highlight-stdint highlight-c">uint32_t</span> i <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-constant highlight-numeric highlight-c++">0</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> i <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> <span class="highlight-constant highlight-numeric highlight-c++">4</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> i<span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
    <span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++">i</span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
</span></pre>
<p>that will compile to code equivalent to:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">0</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">1</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">2</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">3</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>The compiler will also often perform this kind of unrolling automatically when it guesses that doing so will improve performance, but using <code>#pragma unroll</code> manually provides a stronger guarantee.</p>
<p>After you’ve written your vector + ILP Mandelbrot implementations for the CPU and GPU, you can include your answer to the following in your final write-up:</p>
<blockquote>
<p><strong>Question 1 for final write-up:</strong> What speedup do you see from increasing the amount of ILP in your CPU and GPU Mandelbrot implementations? What strategy did you use for partitioning the image into groups of vectors, and why did you choose it? How did you deal with managing control flow and state on the CPU and GPU? How many different vectors did you choose to process at once in your CPU and GPU implementations, and why? What seem to be the limiting factors on how far you can scale ILP?</p>
</blockquote>
<h2>Part 2: Multi-Core Parallelism</h2>
<p>So far in Lab 1 and Lab 2, we’ve seen how to push the limits of performance for a <em>single thread</em> running on a <em>single core</em> on both CPUs and GPUs, by exploiting a combination of vector parallelism and instruction-level parallelism. Now it’s finally time to start scaling our programs in the other direction, to run across <strong>multiple cores</strong>.</p>
<p>In this section, we’ll look at how multi-core parallelism is exposed at the software level on CPUs and on GPUs, and implement <strong>corresponding</strong> Mandelbrot algorithms for both platforms that run <strong>one thread on each core</strong> of their respective machines.</p>
<p>Note that, for the sake of simplicity, we’ll be <strong>setting aside our ILP-optimized implementations</strong> from the previous section for now, and working from our <strong>original vector-parallel</strong> implementations as a starting point instead. We’ll pick the ILP-optimized implementations back up again in Part 4.</p>
<h3>Implementation: CPU Multi-Core</h3>
<p>On the CPU, we’ll be accessing multi-core parallelism via the <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html">POSIX thread (“pthread”) API</a>, which you may have previously encountered in another computer systems or performance engineering class. Our goal is to use the pthread API to:</p>
<ol>
<li>
<p>Spawn <strong>8 child CPU threads</strong>, one for each core of the machine.</p>
</li>
<li>
<p>Run a <strong>vector-parallel</strong> computation on each of those 8 threads to compute the Mandelbrot fractal.</p>
</li>
<li>
<p><strong>Synchronize</strong> on completion of the 8 threads, so that we don’t return until we’re sure that all 8 threads have finished.</p>
</li>
</ol>
<p>This leads to the first deliverable of Part 2:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore</code> as described.</p>
</blockquote>
<p>For your implementation, you will likely want to look at the functions <a href="https://man7.org/linux/man-pages/man3/pthread_create.3.html"><code>pthread_create(...)</code></a> and <a href="https://man7.org/linux/man-pages/man3/pthread_join.3.html"><code>pthread_join(...)</code></a>.</p>
<p>You may want to think about different ways that you could <strong>partition work</strong> between the threads you’re spawning. How do you plan to decide what subset of the image each thread will be responsible for?</p>
<blockquote>
<p><strong>Question 2 for final write-up:</strong> What speedup over the single-core vector-parallel CPU implementation do you see from parallelizing over 8 cores? How do you think the work partitioning strategy might affect the end-to-end performance of the program?</p>
</blockquote>
<h3>Implementation: GPU Multi-Core</h3>
<p>By direct analogy to running one (vector-parallel) thread on each of the 8 cores of the CPU, on the GPU our goal for this section is to run one <strong>warp</strong> on each of the GPU’s <strong>192 warp schedulers</strong>.</p>
<p>Conceptually, what we want to do is to launch our kernel with <code>192 * 32 = 6144</code> CUDA threads, and have each group of 32 of those CUDA threads run on a separate warp scheduler. However, we can’t just launch our kernel with launch parameters</p>
<pre><code>&lt;&lt;&lt;1, 6144&gt;&gt;&gt;
</code></pre>
<p>because CUDA uses a <strong>two-level hierarchy of software parallelism</strong> which corresponds roughly to the two-level  hierarchy of SMs and warp schedulers which is present in the hardware. Specifically:</p>
<ol>
<li>
<p><strong>CUDA threads are organized into groups called “blocks.”</strong> <sup class="footnote-reference"><a href="#cta">3</a></sup> When you launch a kernel, you specify both the number of blocks to launch, and the number of CUDA threads to launch per block. These are the two numbers in the <code>&lt;&lt;&lt;..., ...&gt;&gt;&gt;</code> kernel launch angle bracket notation.</p>
</li>
<li>
<p><strong>All the warps in a given block are guaranteed to run on the same SM.</strong> It is possible to have multiple blocks run on the same SM simultaneously, but not vice-versa.</p>
</li>
</ol>
<p>Like SMs themselves, blocks exist mostly because of the memory system; blocks define from the software side the level at which warps share certain memory resources, in the same way that, on the hardware side, SMs define the level at which warp schedulers share certain memory resources. (In this lab, we’re not concerned with the memory system, but we hope providing this context helps provide some clarity about why blocks exist in the first place.)</p>
<p>The existence of blocks means that if you want to use more than one SM, you need to launch more than one block. In our case, we want to use all 48 SMs on the GPU, and all 4 warp schedulers on each SM. One reasonable way to achieve this is to launch a kernel with launch configuration <code>&lt;&lt;&lt;48, 4 * 32&gt;&gt;&gt;</code>, which will assign one block to each SM, and then assign the 4 warps inside that block to the SM’s 4 warp schedulers.</p>
<p>This gives us almost all the pieces we need to scale up our Mandelbrot kernel to use every warp scheduler on the GPU. There are just two more things you strictly need to know:</p>
<ol>
<li>
<p>The value of <code>threadIdx.x</code> is unique per CUDA thread within a block, and resets across blocks. That means that in any given block, warp <code>0</code> will have <code>threadIdx.x</code> values <code>0, ..., 31</code>, warp <code>1</code> will have <code>threadIdx.x</code> values <code>32, ..., 63</code>, and so on. <sup class="footnote-reference"><a href="#3D">4</a></sup></p>
</li>
<li>
<p>You can access the index of the current block using the magic variable <code>blockIdx.x</code>.</p>
</li>
</ol>
<p>Finally, you may find it helpful to know that you can access the total number of blocks the kernel was launched with using the variable <code>grimDim.x</code>, and access the number of CUDA threads per block with the variable <code>blockDim.x</code>.</p>
<p>We’re now ready to write the multi-core vector-parallel GPU Mandelbrot implementation:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_gpu_vector_multicore</code> and <code>launch_mandelbrot_gpu_vector_multicore</code> to run exactly one warp on each warp scheduler of the GPU.</p>
</blockquote>
<p>As in the CPU case, you may want to consider different partitioning strategies for deciding which subset of the image each warp will be responsible for.</p>
<blockquote>
<p><strong>Question 3 for final write-up:</strong> What speedup over the single-warp vector-parallel GPU implementation do you see from parallelizing over 192 warp schedulers? How does the absolute run time of the GPU multi-core version compare to the CPU multi-core version? How did you approach designing the work partitioning strategy?</p>
</blockquote>
<blockquote>
<p><strong>Question 4 for final write-up:</strong> Try adapting the kernel to use a launch configuration of <code>&lt;&lt;&lt;96, 2 * 32&gt;&gt;&gt;</code>. Will this still assign exactly one warp to every warp scheduler on the machine? How about <code>&lt;&lt;&lt;24, 8 * 32&gt;&gt;&gt;</code>? How do the run times of all of these configurations compare to each other?</p>
</blockquote>
<h2>Part 3: Multi-Threaded Parallelism</h2>
<p>Even though we’re now running code on every core of both our CPU and GPU, there is still one more level of parallelism left to exploit. As we saw at the beginning of this lab, both the cores on our CPU and the warp schedulers on our GPU support <strong>multi-threading</strong>, allowing for concurrent execution of instructions from multiple threads on a single core. By spawning more threads than we have cores, we can start to take advantage of the additional opportunities for parallelism exposed by multi-threading.</p>
<h3>Implementation: CPU Multi-Threaded</h3>
<p>We can adapt our multi-core CPU implementation to use multiple threads per core simply by increasing the number of threads:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore_multithread</code> to spawn <code>&gt; 1</code> thread per CPU core. As before, every thread in this new implementation should continue to exploit vector parallelism. You can choose the exact number of threads spawned as you see fit.</p>
</blockquote>
<blockquote>
<p><strong>Question 5 for final write-up:</strong> How much are you able to speed up your implementation by introducing multi-threading per-core? What seems to be the optimal number of threads to spawn? What factors do you think might contribute to determining the optimal number of threads?</p>
</blockquote>
<h3>Implementation: GPU Multi-Threaded</h3>
<p>As in the CPU case, we can introduce multi-threading per-core into our multi-core GPU implementation just by launching more warps than we have warp schedulers across the machine.</p>
<p>Before we do that, however, it may be instructive to look specifically at how performance varies as a function of number of warps <strong>if we restrict the kernel’s execution to a single block on a single SM</strong>, which allows us to get a particularly clean view of the effect of multi-threading without needing to worry about effects related to block-level scheduling and workload assignment:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_cpu_vector_multicore_multithread_single_sm</code> and <code>launch_cpu_vector_multicore_multithread_single_sm</code> to run as many warps as you want (up to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability">hardware limit</a> of <code>32</code> warps per block), but all inside just a <strong>single block</strong>.</p>
</blockquote>
<blockquote>
<p><strong>Question 6 for final write-up:</strong> In the <code>mandelbrot_cpu_vector_multicore_multithread_single_sm</code> kernel, how does run time vary as a function of the number of warps, beyond the point where there is one warp to populate each of the 4 warp schedulers on the SM? Does it keep improving all the way up to the hard limit of <code>32</code> warps per block? If so, by how much? What factors do you think might be contributing to what you observe?</p>
</blockquote>
<p>Returning to a more realistic setting, analogous to the multi-threaded CPU implementation, we can finally write our full-scale multi-threaded GPU implementation:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_gpu_vector_multicore_multithread_full</code> and <code>launch_mandelbrot_gpu_vector_multicore_multithread_full</code> to run as many warps as you want across as many blocks as you want.</p>
</blockquote>
<blockquote>
<p><strong>Question 7 for final write-up:</strong> As in the CPU case: How much are you able to speed up your GPU implementation by introducing multi-threading per-warp-scheduler? What seems to be the optimal number of warps to spawn? What factors do you think might contribute to determining the optimal number of warps?</p>
</blockquote>
<h2>Part 4: Putting It All Together</h2>
<p>Now that we have seen how to saturate both the CPU and GPU’s capacity for vector parallelism, multi-core parallelism, and multi-threaded parallelism, there is just one remaining thing left to try, which is to <strong>add back in the ILP optimizations</strong> we introduced in Part 1 and discarded in Part 2:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore_multithread_ilp</code> by combining the techniques you used to increase ILP in <code>mandelbrot_cpu_vector_ilp</code> with the techniques you used to achieve multi-core and multi-threaded parallelism in <code>mandelbrot_cpu_vector_multicore_multithread</code>.</p>
</blockquote>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, do the same for your GPU implementations in the functions <code>mandelbrot_gpu_vector_multicore_multithread_full_ilp</code> and <code>launch_mandelbrot_gpu_vector_multicore_multithread_full_ilp</code>.</p>
</blockquote>
<blockquote>
<p><strong>Question 8 for final write-up:</strong> In your CPU and GPU implementations, how much speedup, if any, were you able to achieve by adding your ILP optimizations back in on top of your multi-core, multi-threaded algorithms? How does the speedup from increasing ILP in this setting compare to the speedup from increasing ILP in the single-threaded, single-core setting? What seems to be the optimal number of threads in this setting on CPU and GPU, and what is the optimal number of independent vectors of pixels to process at once in the inner loop? What factors do you think might be contributing to what you observe?</p>
</blockquote>
<h2>Extensions &amp; Extra Credit</h2>
<ul>
<li>
<p>In addition to the <code>FFMA</code>, measure latency of other instructions. Brownie points if it interacts with the memory subsystem.</p>
</li>
<li>
<p>The curve we generated with <code>warp_scheduler.cu</code> is reminiscent of what we call a roofline curve <sup class="footnote-reference"><a href="#4">5</a></sup>. Can you empirically generate the entire roofline curve for this GPU?</p>
</li>
</ul>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Some NVIDIA documentation refers to these four elements of the SM as “partitions,” and uses the term “warp scheduler” to refer to just the control logic of a partition. Our use of the term “warp scheduler” to refer to the whole unit is therefore a kind of <a href="https://en.wikipedia.org/wiki/Synecdoche">synecdoche</a>. Other authors refer to these elements of the SM as “<a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">quadrants</a>.”</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>In particular, the cores on the CPU we’re using are <a href="https://en.wikipedia.org/wiki/Wide-issue">wide-issue</a> and <a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order</a>, whereas the cores on our GPU are single-issue and in-order.</p>
</div>
<!-- [^latency-source]: [Source](https://docs.nvidia.com/cuda/turing-tuning-guide/index.html#instruction-scheduling) for the 4-cycle latency figure. The linked document was written for slightly older hardware than we're using, but should still apply.

[^throughput-source]: [Source](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions) for the 1 instruction per cycle throughput figure. In the linked table, the "compute capability" column corresponding to the specific GPU we're using is "8.6." Note that this table lists throughput per **SM**, which is the aggregate throughput across its 4 warp schedulers. Generally dividing these numbers by 4 gives the throughput per warp scheduler. -->
<div class="footnote-definition" id="cta"><sup class="footnote-definition-label">3</sup>
<p>“Blocks” are sometimes also called “cooperative thread arrays” (CTAs), and NVIDIA’s documentation uses the two terms interchangeably.</p>
</div>
<div class="footnote-definition" id="3D"><sup class="footnote-definition-label">4</sup>
<p>We’re deliberately ignoring the <code>y</code> and <code>z</code> dimensions of CUDA’s <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy">3D parallelism</a> in this lab, because 3D parallelism is essentially just a thin layer of user-interface convenience over what is fundamentally a <em>linear</em> index space.</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">5</sup>
<p>For more reading, you can refer to the original <a href="https://dl.acm.org/doi/pdf/10.1145/1498765.1498785">roofline article</a> from Patterson et al.</p>
</div>
</main><footer><p><a href="https://mit.edu">Massachusetts Institute of Technology</a> —
<a href="https://www.eecs.mit.edu">Department of Electrical Engineering and Computer Science</a> | <a href="https://accessibility.mit.edu">Accessibility</a></p>
</footer></body></html>