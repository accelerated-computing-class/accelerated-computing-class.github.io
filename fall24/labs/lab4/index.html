<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>6.S894</title><base href="/fall24/labs/lab4/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/fall24/assets/main.css" rel="stylesheet"><link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet"><link href="/fall24/assets/favicon.png" rel="icon" type="image/png"></head><body><header><nav><h1><a href="/fall24/">6.S894</a></h1>
<p><a href="/fall24/calendar">Calendar</a></p>
<p><a href="/fall24/labs">Labs</a></p>
<p><a href="/fall24/syllabus">Syllabus</a></p>
<p><a href="/fall24/resources">Resources</a></p>
<p><a href="/fall24/contact">Contact</a></p>
<p><a href="/fall24/piazza">Piazza</a></p>
</nav></header><main><h1>Lab 4: Matrix Multiply – Tiling and Reuse</h1>
<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint:</strong> Due Monday, September 30, 11:59pm (<a href="https://www.gradescope.com/courses/849967/assignments/5072213/">Gradescope</a>)</p>
</li>
<li>
<p><strong>Final Submission:</strong> Due Friday, October 4, 11:59pm (<a href="https://www.gradescope.com/courses/849967/assignments/5072328/">Gradescope</a>)</p>
</li>
</ul>
<p>See the “Deliverables” section at the end of this document for more information on what you’ll be turning in.</p>
<p>Note that because we pushed both the Lab 3 deadline and Lab 4 release back by one day relative to the usual schedule, the checkpoint for this week will be relatively relaxed.</p>
<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab4">lab repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab4.git</span>
</span></pre>
<h3>Errata: L1 Cache Behavior (<a href="/fall24/resources/errata/l1-cache">Link</a>)</h3>
<p>Since publishing this lab, we’ve seen evidence that some aspects of our description of the L1 cache may be inaccurate.</p>
<p><strong>All the performance guidance in this lab continues to apply</strong>, in the sense that, if you follow that guidance, your code will run fast in the ways we’ve described.</p>
<p>However, it seems that NVIDIA’s L1 cache hardware may have capabilities beyond what we gave it credit for, and that some programming patterns which we warn against may in fact perform <strong>better than expected</strong>. For more information, see <a href="/fall24/resources/errata/l1-cache"><em>Errata: L1 Cache Behavior</em></a>.</p>
<h3>Viewing Assembly</h3>
<p>Modern compilers are complicated beasts, and the best way to understand what they’re doing is often to manually inspect the instructions they emit. Starting with this lab, and going forward, we encourage you to start making a habit of <strong>looking at the generated assembly code</strong> for your kernels as part of your performance engineering workflow.</p>
<p>To view the assembly code for your GPU kernels, you can use <a href="https://godbolt.org/z/Pbf494Tzd">Compiler Explorer</a> (also known as “Godbolt,” after its creator <a href="https://xania.org/MattGodbolt">Matt Godbolt</a>). Compiler Explorer is an excellent free online tool which lets you view the assembly code for programs written in any of a huge variety of different languages, compiled with any of a huge selection of different compilers.</p>
<p>You can use the following Compiler Explorer link to interactively view the assembly for your CUDA programs:</p>
<p>&gt;&gt;&gt; <a href="https://godbolt.org/z/Pbf494Tzd">Compiler Explorer for CUDA</a> &lt;&lt;&lt;</p>
<p>(We’ve tried to configure this Compiler Explorer page to approximate the CUDA build environment we use on Telerun as closely as possible, but it’s possible there will be some discrepancies. <del>We’re working on letting you also get assembly output directly from Telerun – stay tuned for that!</del>)</p>
<blockquote>
<p><strong>Update 2024-10-01:</strong> Telerun now supports assembly output! Pull from the <a href="https://github.com/accelerated-computing-class/telerun">Telerun Git repo</a> to update your client to the latest version.</p>
<p>Use the <code>--asm</code> flag (shorthand <code>-s</code>) to save assembly output for your program:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">python3</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> telerun.py submit<span class="highlight-variable highlight-parameter highlight-option highlight-shell"><span class="highlight-punctuation highlight-definition highlight-parameter highlight-shell"> --</span>asm</span> source_file.cu</span>
</span></pre>
<p>You can find the generated assembly code in <code>./telerun-out/&lt;job_id&gt;/</code>.</p>
</blockquote>
<p>In Compiler Explorer, you’ll find that the CUDA compiler actually generates two different kinds of assembly code for your kernels, which you can toggle between with a drop-down menu: “<strong>PTX</strong>,” and “<strong>SASS</strong>.” SASS corresponds to the actual machine code which the GPU executes, whereas PTX corresponds to a slightly higher-level representation of your program.<sup class="footnote-reference"><a href="#llvm">1</a></sup> Whenever a CUDA program is compiled and executed on a GPU, it is first compiled to PTX, and then that PTX is compiled to SASS.</p>
<p>The PTX/SASS distinction exists because NVIDIA wants to reserve the right to change the ISA they use in their hardware over time. As such, SASS is <strong>unstable</strong> across different generations of GPUs, essentially <strong>undocumented</strong>, and <strong>cannot be authored by hand</strong> (NVIDIA does not provide an official SASS-text-to-binary assembler). PTX, by contrast, is stable, thoroughly documented, and intended to be written by hand (when the need arises).</p>
<p>We won’t be going too in-depth on exactly how PTX and SASS work, but it’s often possible to guess roughly what a PTX or SASS instruction does based just on its name. For more information, you can refer to NVIDIA’s documentation here:</p>
<ul>
<li>
<p><strong>PTX Documentation:</strong> <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html">Link</a></p>
</li>
<li>
<p><strong>SASS Instruction List:</strong> <a href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#nvidia-ampere-gpu-and-ada-instruction-set">Link</a></p>
</li>
</ul>
<h2>Introduction</h2>
<h3>Goals for This Lab</h3>
<p>Modern GPUs are extremely flexible, but one of the things they’re best at – and best known for – is <strong>matrix multiplication</strong>. Among many other applications, matrix multiplication plays a central role in <strong>deep learning</strong>, where it serves as <em>the</em> fundamental computational primitive underlying training and inference for neural networks like <a href="https://claude.ai/">large language models</a> and <a href="https://www.midjourney.com/explore">diffusion image generators</a>. As an example of the importance of this application, you may have seen in the news that Meta <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">recently purchased 350000 NVIDIA H100 GPUs</a> to “support their current and next generation AI models.” What that means in practice is that those 350000 GPUs will be spending most of their operational life multiplying matrices.</p>
<p>Over the course of the <strong>next several weeks</strong>, we’ll be learning how to implement a <strong>nearly state-of-the-art</strong> matrix multiplication kernel for NVIDIA GPUs. We’ll be building up our implementation piece-by-piece, with a new lab each week covering a new set of optimizations. By the end of this progression, we’ll be using advanced features of the GPU like <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/#introducing_the_nvidia_a100_tensor_core_gpu">tensor cores</a> and <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/#asynchronous_copy%C2%A0">asynchronous copies</a>. For this first lab, though, we’ll only need the features of CUDA that we’ve already seen in Labs 1 - 3.</p>
<p>Concretely, in this lab we’ll be looking at three ideas:</p>
<ol>
<li>
<p>The importance of <strong>data reuse</strong> in matrix multiplication.</p>
</li>
<li>
<p>How to exploit data reuse at the level of the <strong>L1 cache</strong> / <strong>shared memory</strong>.</p>
</li>
<li>
<p>How to exploit data reuse at the level of the <strong>register file</strong>.</p>
</li>
</ol>
<h2>Part 1: Analysis</h2>
<p>The matrix multiplication workload we’ll be implementing can be described by the following sequential code:</p>
<pre><span class="highlight-source highlight-python"><span class="highlight-comment highlight-line highlight-number-sign highlight-python"><span class="highlight-punctuation highlight-definition highlight-comment highlight-python">#</span> a: float array of size `size_i * size_k`
</span><span class="highlight-comment highlight-line highlight-number-sign highlight-python"><span class="highlight-punctuation highlight-definition highlight-comment highlight-python">#</span> b: float array of size `size_k * size_j`
</span><span class="highlight-comment highlight-line highlight-number-sign highlight-python"><span class="highlight-punctuation highlight-definition highlight-comment highlight-python">#</span> c: float array of size `size_i * size_j`
</span><span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-python">for</span> <span class="highlight-meta highlight-generic-name highlight-python">i</span> <span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-in highlight-python">in</span></span></span><span class="highlight-meta highlight-statement highlight-for highlight-python"> <span class="highlight-meta highlight-function-call highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-support highlight-function highlight-builtin highlight-python">range</span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-begin highlight-python">(</span><span class="highlight-meta highlight-function-call highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">size_i</span></span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-end highlight-python">)</span></span><span class="highlight-punctuation highlight-section highlight-block highlight-for highlight-python">:</span></span>
    <span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-python">for</span> <span class="highlight-meta highlight-generic-name highlight-python">j</span> <span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-in highlight-python">in</span></span></span><span class="highlight-meta highlight-statement highlight-for highlight-python"> <span class="highlight-meta highlight-function-call highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-support highlight-function highlight-builtin highlight-python">range</span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-begin highlight-python">(</span><span class="highlight-meta highlight-function-call highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">size_j</span></span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-end highlight-python">)</span></span><span class="highlight-punctuation highlight-section highlight-block highlight-for highlight-python">:</span></span>
        <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">s</span></span> <span class="highlight-keyword highlight-operator highlight-assignment highlight-python">=</span> <span class="highlight-constant highlight-numeric highlight-integer highlight-decimal highlight-python">0</span>
        <span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-python">for</span> <span class="highlight-meta highlight-generic-name highlight-python">k</span> <span class="highlight-meta highlight-statement highlight-for highlight-python"><span class="highlight-keyword highlight-control highlight-flow highlight-for highlight-in highlight-python">in</span></span></span><span class="highlight-meta highlight-statement highlight-for highlight-python"> <span class="highlight-meta highlight-function-call highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-support highlight-function highlight-builtin highlight-python">range</span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-begin highlight-python">(</span><span class="highlight-meta highlight-function-call highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">size_k</span></span></span><span class="highlight-punctuation highlight-section highlight-arguments highlight-end highlight-python">)</span></span><span class="highlight-punctuation highlight-section highlight-block highlight-for highlight-python">:</span></span>
            <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">s</span></span> <span class="highlight-keyword highlight-operator highlight-assignment highlight-augmented highlight-python">+=</span> <span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">a</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-begin highlight-python">[</span></span><span class="highlight-meta highlight-item-access highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">i</span></span>, <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">k</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-end highlight-python">]</span></span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-python">*</span> <span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">b</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-begin highlight-python">[</span></span><span class="highlight-meta highlight-item-access highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">k</span></span>, <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">j</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-end highlight-python">]</span></span>
        <span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">c</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-begin highlight-python">[</span></span><span class="highlight-meta highlight-item-access highlight-arguments highlight-python"><span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">i</span></span>, <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">j</span></span></span><span class="highlight-meta highlight-item-access highlight-python"><span class="highlight-punctuation highlight-section highlight-brackets highlight-end highlight-python">]</span></span> <span class="highlight-keyword highlight-operator highlight-assignment highlight-python">=</span> <span class="highlight-meta highlight-qualified-name highlight-python"><span class="highlight-meta highlight-generic-name highlight-python">s</span></span>
</span></pre>
<p>Because this matrix multiplication does not assume any special structure or sparsity in the input matrices, it’s an example of what’s sometimes called a “general matrix multiplication,” or “<a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">gemm</a>.”</p>
<p>In future weeks, we’ll be varying the values of <code>size_i</code>, <code>size_j</code>, <code>size_k</code>, but for now we only care about <strong>two problem sizes</strong>:</p>
<ul>
<li>
<p>A <strong>small-scale test case</strong> with <code>size_i = size_j = size_k = 256</code></p>
</li>
<li>
<p>A <strong>large-scale benchmark</strong> with <code>size_i = size_j = size_k = 3072</code></p>
</li>
</ul>
<p>For the purposes of this lab, we don’t care about performance on the small-scale tests, only the large-scale benchmark. So for any performance analysis, you can assume every matrix is square with side length <code>3072</code>.</p>
<p>Before we start writing code to implement this workload on the GPU, it will be useful to have the answers to a few questions in mind (see further below for relevant numbers you might find helpful for answering these):</p>
<blockquote>
<p><strong>Question 1 for final write-up:</strong> Walk through the following analysis in the context of the large-scale tests, with matrices of size <code>3072</code>:</p>
<ol>
<li>
<p>How many FLOPs are we computing in total? (Each <code>s += a[i, k] * b[k, j]</code> is a single “fused multiply-add” (FMA), which is conventionally counted as <strong>two</strong> FLOPs.)</p>
</li>
<li>
<p>If FLOPs were the only constraint, what is the fastest this workload could possibly run on our GPU? (Assume we’re using ordinary FMA instructions, not tensor cores.)</p>
</li>
<li>
<p>How many <strong>unique</strong> locations in DRAM are we accessing over the whole workload? (Assume the “A” and “B” matrices start in DRAM, and that our results in “C” must be written to DRAM.)</p>
</li>
<li>
<p>If we only needed to access each unique location in DRAM <strong>once</strong>, and if DRAM bandwidth were the only constraint, what is the fastest this workload could possibly run on our GPU?</p>
</li>
<li>
<p>How does (4) compare to (2)? Given a very well-optimized implementation, would you expect the run time of this workload to be dominated by compute or by data movement?</p>
</li>
<li>
<p>Alternatively, imagine we <strong>do not exploit reuse</strong>, so every <code>s += a[i, k] * b[k, j]</code> operation loads the <code>a[i, k]</code> and <code>b[k, j]</code> elements <strong>directly from DRAM</strong>. Then how many total bytes would we need to load from DRAM?</p>
</li>
<li>
<p>If we had no reuse, as in (6), and if DRAM bandwidth were the only constraint, what is the fastest this workload could possibly run on our GPU?</p>
</li>
<li>
<p>Imagine instead that every <code>s += a[i, k] * b[k, j]</code> operation could somehow load directly from <strong>L2</strong>. Then if L2 bandwidth were the only constraint, what is the fastest this workload could possibly run?</p>
</li>
<li>
<p>How do (7) and (8) compare to (2)?</p>
</li>
</ol>
</blockquote>
<p><strong>Relevant Numbers:</strong> You may find the following hardware specs for our RTX A4000 GPU helpful in answering the questions above:</p>
<ul>
<li>
<p><strong>Total fused multiply-add (FMA) throughput:</strong> <br></p>
<pre><code>  (32 lanes / warp scheduler / cycle)
* (2 FLOP / lane)
* (4 warp schedulers / SM)
* (48 SMs)
* (1.56 GHz)

= 19.2 TFLOP/sec
</code></pre>
</li>
<li>
<p><strong>Total DRAM bandwidth:</strong> <br></p>
<pre><code>448 GB/sec
</code></pre>
</li>
<li>
<p><strong>Total L2 bandwidth:</strong> <br></p>
<pre><code>1.5 TB/sec
</code></pre>
</li>
</ul>
<!--
* **Total SM-level SRAM bandwidth:** <br>
    ```
      (32 lanes / SM / cycle)
    * (4 bytes / lane)
    * (48 SMs)
    * (1.56 GHz)

    = 9.6 TB/sec
    ``` -->
<h2>Part 2: Reuse in L1</h2>
<p>Our first implementation goal in this lab will be to write a matrix multiplication kernel which runs in <strong>under 80 ms</strong> on our large-scale benchmark.</p>
<p>If you’ve already figured out your answers for Question 1 in Part 1, you might notice something interesting about this 80 ms target: it is <strong>impossible to achieve</strong> if the operands for every FMA are loaded directly from <strong>L2</strong> or <strong>DRAM</strong>. That means that we’re going to need to <strong>amortize</strong> the cost of loading data from L2 and DRAM, and load the operands for most of our FMAs from <strong>faster memory resources</strong>, such as the L1 SRAM on each SM.<sup class="footnote-reference"><a href="#l1_terminology">2</a></sup> To do that, we’ll need to exploit <strong>data reuse</strong>!</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>matmul.cu</code>, implement the functions <code>matmul_l1</code> and <code>launch_matmul_l1</code> so that they can process the large-scale benchmark in <strong>under 80 ms</strong>. To do this, your program will need to somehow make use of local SRAM resources.</p>
</blockquote>
<p>Below, we give some suggestions on how you might achieve this performance target. (You may find it’s easier than you expect!)</p>
<h3>Partitioning Strategies</h3>
<p>There are many possible strategies for partitioning matrix multiplications across parallel processors, but we recommend you adopt a particularly popular and effective partitioning strategy called an <strong>“output-stationary dataflow”</strong>.</p>
<p>An output-stationary dataflow is one in which any given parallel worker in your program focuses on computing a <strong>single</strong> region of the output matrix C at a time, while iterating over <strong>multiple</strong> regions of the input matrices A and B. In the simplest form of output-stationary matrix multiplication on a GPU, every <strong>block</strong> in your kernel launch is responsible for fully computing the values of an <strong>independent</strong> subset of elements in the output matrix C – usually a square or rectangular tile. This will result in each block reading a rectangular strip of values from each of the A and B matrices.</p>
<p>It’s often useful to view the three-dimensional <code>i, j, k</code> iteration space of a matrix multiplication as a <strong>cuboid</strong>, with each of the three matrices A, B, C corresponding to a different <strong>plane</strong>: the A matrix being the <code>i, k</code> plane, the B matrix the <code>k, j</code> plane, and the C matrix the <code>i, j</code> plane. In such a representation, we can visualize our simple output-stationary dataflow as follows:</p>
<p><img src="images/gemm_cube.png" alt="Matrix Multiply Cube" /></p>
<p>We recommend keeping the values of the in-progress C tile in <strong>registers</strong> for the duration of each block’s execution. Keeping the partial sum for each element of the C tile in registers allows you to very efficiently <strong>additively accumulate</strong> new values into it as you iterate over different parts of the A and B matrices.</p>
<p>If you hit any performance problems related to keeping your C tile values in registers, you may find it helpful to read ahead to the section “Using Registers Effectively” in Part 3.</p>
<h3>Data Reuse Strategies</h3>
<p>As we discussed in <a href="/fall24/labs/lab3">Lab 3</a>, there are a few different ways a CUDA program can make use of the L1 SRAM on an SM:</p>
<ol>
<li>
<p><strong>As an explicit shared memory scratchpad.</strong> <br>
This is how we accessed the L1 in Lab 3. You can declare a shared memory scratchpad with <code>extern __shared__</code>, and then arbitrarily write to it and read from it as you see fit.</p>
</li>
<li>
<p><strong>As a cache for read-only data.</strong> <br>
This is more similar to how the L1 cache works on a CPU, in which the cache <strong>automatically</strong> saves the value at a given address on first access, and then <strong>automatically</strong> reuses that value on subsequent accesses until it is evicted. Because the L1 caches on the GPU are <a href="https://en.wikipedia.org/wiki/Cache_coherence"><strong>incoherent</strong></a>, the compiler will typically emit load instructions which <strong>bypass</strong> the L1 cache by default, and will only emit instructions that use the L1 cache under two circumstances:</p>
<ol>
<li>
<p>If the <strong>compiler can figure out</strong> that the memory location you’re reading won’t change from one access to the next. This is somewhat rare and unreliable, but it <em>can</em> happen!</p>
</li>
<li>
<p>If you <strong>manually tell the compiler</strong> that the memory location you’re reading won’t change from one access to the next. You can do this by using the special <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#read-only-data-cache-load-function"><code>__ldg(...)</code></a> function.</p>
</li>
</ol>
</li>
</ol>
<p>You’re welcome to try using any of these methods to make use of the L1 in your matrix multiplication kernel. You may find it helpful to keep in mind that the capacity of the L1 is <strong>128 KB</strong> per SM, of which at most <strong>100 KB</strong> can be provisioned as shared memory.</p>
<p>If you hit any performance problems related to achieving data reuse in the L1 SRAM, you may want to read ahead to the section “Using L1 Effectively” in Part 3.</p>
<h3>Questions</h3>
<p>Once your matrix multiplication kernel is working, you can answer the following in your write-up:</p>
<blockquote>
<p><strong>Question 2 for final write-up:</strong> What run time were you able to achieve with this first matrix multiplication kernel? How did you choose your output tile size, and what factors do you think influence the optimal choice of tile size? How did you achieve reuse via the L1? If you used the L1 as shared memory, what was your strategy? If you used the L1 as a read-only cache, can you find evidence in your program’s PTX assembly that your program’s load instructions are indeed configured to use the cache?</p>
</blockquote>
<p><strong>Tip:</strong> In PTX assembly, a load from global memory which uses the L1 cache is written as <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-ld-global-nc"><code>ld.global.nc</code></a> (the “<code>nc</code>” stands for “non-coherent”).</p>
<h2>Part 3: Reuse in Registers</h2>
<h3>Analysis Revisited</h3>
<p>To understand what we need to do to push our matmul implementation further, we can continue the analysis from Part 1 with one more thought experiment:</p>
<blockquote>
<p><strong>Question 3 for final write-up:</strong> Imagine that every load of <code>a[i, k]</code> or <code>b[k, j]</code> could somehow load <strong>directly from L1</strong>. Then if L1 bandwidth were the only constraint, what is the fastest our workload could possibly run?</p>
</blockquote>
<p>For reference, the <strong>aggregate L1 bandwidth</strong> across the whole machine is given by: <br></p>
<pre><code>  (32 lanes / SM / cycle)
* (4 bytes / lane)
* (48 SMs)
* (1.56 GHz)

= 9.6 TB/sec
</code></pre>
<h3>Implementation</h3>
<p>For our second and final kernel in this lab, we’ll be aiming for a much more ambitious run time target than in the previous section: <strong>16 ms or less</strong> on our large-scale benchmark. If you’ve already answered Question 3, you might notice that in order to hit this 16 ms target, we’re going to need to load the operands for our FMAs from some memory resource which is <strong>faster than L1</strong>. But what’s faster than L1? There’s only one such resource: <strong>registers!</strong></p>
<p>In the same way that in Part 2 we amortized the cost of loads from L2 and DRAM by reusing data in L1, in this kernel we’ll need to amortize the cost of loading from L1 by reusing data in registers.</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>matmul.cu</code>, implement the functions <code>matmul_l1_reg</code> and <code>launch_matmul_l1_reg</code> so that they can process the large-scale benchmark in <strong>under 16 ms</strong>. To do this, your program will need to somehow reuse data at the level of the register file.</p>
</blockquote>
<p>The strategy we recommend for implementing this kernel is to decompose the problem into <em>microtiles</em>, such that we can compute the FMAs associated with each microtile <em>entirely using data stored (temporarily) in registers</em>. Using our three-dimensional visualization of the <code>i, j, k</code> iteration space from earlier, such a microtile decomposition would look something like the following:</p>
<p><img src="images/gemm_block_microtiles.png" alt="Matrix multiply decomposition into microtiles" /></p>
<p>(Note that this diagram is not quite to scale – you’ll probably want to have more than <code>3 * 3</code> microtiles per block along the <code>i</code> and <code>j</code> dimensions!)</p>
<p>Below, we discuss a few performance considerations which you may want to keep in mind when writing your kernel.</p>
<h4>Using Registers Effectively</h4>
<p>To make effective use of the register file on the GPU, there are a few things it might be helpful to know:</p>
<ul>
<li>
<p>The register file has a capacity of <strong>64 KB</strong> per <strong>warp scheduler</strong>, or, equivalently, <strong>256 KB</strong> per <strong>SM</strong>.</p>
</li>
<li>
<p>The compiler will try to put “<strong>stack variables</strong>,” like <code>int i;</code> or <code>float x[4][4];</code>, <strong>in registers by default</strong>. However, there are three main things which can interfere with this process, in which case variables will “spill to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">local memory</a>”:</p>
<ol>
<li>
<p>If your stack variables <strong>don’t all fit</strong> in the register file, some of them will spill.</p>
</li>
<li>
<p>If you access an array using a <strong>dynamic index</strong>, the array will spill. (E.g. writing <code>a[i]</code> where the compiler cannot statically resolve <code>i</code> to a constant.)</p>
</li>
<li>
<p>If you <strong>take the address</strong> of a stack variable in such a way that the compiler can’t translate it to an equivalent form that doesn’t involve taking the address, the variable will spill. (E.g. writing <code>f(&amp;x)</code> where <code>f</code> is an opaque function which is not inlined.)</p>
</li>
</ol>
</li>
<li>
<p>Data which spills to local memory is <strong>cached</strong> in L1 and L2 when possible, but is ultimately backed by <strong>DRAM</strong>. If you try to allocate a sufficiently large amount of stack memory, eventually your kernel will end up needing to make round trips to DRAM to access its stack variables.</p>
</li>
<li>
<p>By default, <strong>the compiler chooses how many registers</strong> your kernel will use per warp. Sometimes, the compiler will choose to use so many registers per warp that it constrains the <strong>maximum number of warps</strong> per block, resulting in confusing errors when you try to launch your kernel with too many warps (even if you’re <strong>below the usual limit</strong> of 32 warps per block). To fix this, you need to use the special <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#launch-bounds"><strong><code>__launch_bounds(...)__</code></strong></a> attribute in your kernel definition, which tells the compiler how many CUDA threads per block you intend to use when launching your kernel.</p>
</li>
</ul>
<h4>Using L1 Effectively</h4>
<p>Both when accessing the L1 as a scratchpad and when accessing it as a cache, you should be aware that there are some constraints on the kinds of access patterns which the L1 SRAM can efficiently support:</p>
<ul>
<li>
<p><strong>Scratchpad Mode: Bank Conflicts.</strong></p>
<p>When you’re accessing the L1 as a scratchpad, you can only make use of its full bandwidth if your code is free of so-called “<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x">bank conflicts</a>.”</p>
<p>If we define a “word” to be 4 bytes, a bank conflict occurs when multiple lanes in a warp try to <strong>simultaneously</strong> access <strong>different locations</strong> in shared memory which map to the <strong>same word index mod 32</strong>. For example, if two lanes were to try to access words at byte-offsets <code>128</code> and <code>256</code> in shared memory, then they would be trying to access word indices <code>32</code> and <code>64</code>. Both of those word indices map to word index <code>0</code> mod 32, so the two lanes would have a bank conflict.</p>
<p>Bank conflicts matter because a word’s index mod 32 in shared memory determines the <strong>physical memory bank</strong> in which the word is stored. Each bank can only service one word-sized request per clock cycle, so if multiple lanes in a warp need to access the same bank, those lanes’ requests must be processed <strong>serially</strong> over multiple cycles.</p>
<p>Note that bank conflicts only apply when multiple lanes try to access <em>different</em> addresses mapping to the same bank. If multiple lanes try to access <strong>exactly the same</strong> addresses, there is <strong>no bank conflict</strong>, and no performance penalty.</p>
</li>
<li>
<p><strong>Cache Mode: Cache Line Effects.</strong></p>
<p>When you’re using the L1 as a read-only data cache, you can only make use of its full bandwidth if each warp accesses just a <strong>single contiguous 32-word cache line</strong> in each load instruction. (<a href="https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/memorystatisticscaches.htm">Reference</a>)</p>
<p>When the L1 is acting as a cache, it can only perform a single tag lookup and serve data from a single cache line on each clock cycle. If different lanes in a warp try to <strong>simultaneously</strong> access data belonging to different cache lines, those lanes’ requests will be processed <strong>serially</strong>, and the total time taken will be proportional to the number of distinct cache lines touched.</p>
<p>You might notice that this means that using the L1 as an implicit cache is in an important sense <strong>less flexible</strong> than using the L1 as a shared memory scratchpad. When using the L1 as a scratchpad, we only need to ensure each load instruction is <strong>free from bank conflicts</strong> in order to use the L1’s full bandwidth. When using it as a cache, we need to ensure that each load is <strong>contiguous and 128-byte-aligned</strong>, which is a much stronger constraint.</p>
</li>
</ul>
<h3>Questions</h3>
<p>Once you’ve implemented your optimized kernel, you can answer the final question of the lab:</p>
<blockquote>
<p><strong>Question 4 for final write-up:</strong> What run time were you able to achieve by exploiting register-level reuse? How did you choose to partition work across CUDA threads and across time? Was your partitioning strategy affected at all by considerations related to bank conflicts or cache line effects? Were there any numerical parameters you needed to tune, and if so, how did you choose their values? When you inspect the generated SASS assembly for your kernel, do you see evidence of register-level reuse? Finally, optionally – do you have any ideas for how we might be able to optimize this kernel further in the next few labs?</p>
</blockquote>
<h2>Deliverables</h2>
<h3>Checkpoint (Due Monday, September 30, 11:59pm)</h3>
<p>Because this lab was released a day later than usual, we’re not requiring you to do much for the checkpoint to get full credit. Ideally, we’d like you to <strong>try answering Question 1</strong> in Part 1 (“Analysis”), and <strong>see how far you can get</strong> on the code for Parts 2 and 3.</p>
<p>On the Gradescope assignment for “Lab 4 Checkpoint,” (<a href="https://www.gradescope.com/courses/849967/assignments/5072213/">link</a>) submit your answers to the prompts checking in about how you’re doing with the lab.</p>
<h3>Final Submission (Due Friday, October 4, 11:59pm)</h3>
<p>On the Gradescope assignment “Lab 4 Final,” (<a href="https://www.gradescope.com/courses/849967/assignments/5072328/">link</a>) submit your completed code for <code>matmul.cu</code>, as well as a PDF write-up containing your answers to Questions 1 - 4.</p>
<hr />
<div class="footnote-definition" id="llvm"><sup class="footnote-definition-label">1</sup>
<p>If you’re familiar with <a href="https://llvm.org/docs/LangRef.html">LLVM IR</a>, you can think of PTX as operating at approximately the same level of abstraction. In particular, PTX represents programs in terms of instructions operating on an unbounded set of virtual registers, rather than a finite set of architectural registers. Register allocation is handled in the PTX-to-SASS compilation step.</p>
</div>
<div class="footnote-definition" id="l1_terminology"><sup class="footnote-definition-label">2</sup>
<p>We’ll use the term “L1” generically to refer to the physical SRAM resource on each SM, which can be used as either an explicitly-addressed shared memory or as an automatically-managed L1 cache.</p>
</div>
</main><footer><p><a href="https://mit.edu">Massachusetts Institute of Technology</a> —
<a href="https://www.eecs.mit.edu">Department of Electrical Engineering and Computer Science</a></p>
</footer></body></html>