<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>6.S894</title><base href="/fall24/labs/lab2/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/fall24/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/fall24/assets/main.css" rel="stylesheet"><link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet"><link href="/fall24/assets/favicon.png" rel="icon" type="image/png"></head><body><header><nav><h1><a href="/fall24/">6.S894</a></h1>
<p><a href="/fall24/calendar">Calendar</a></p>
<p><a href="/fall24/labs">Labs</a></p>
<p><a href="/fall24/syllabus">Syllabus</a></p>
<p><a href="/fall24/resources">Resources</a></p>
<p><a href="/fall24/contact">Contact</a></p>
<p><a href="/fall24/piazza">Piazza</a></p>
</nav></header><main><h1>Lab 2: Massively-Parallel Mandelbrot</h1>
<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint</strong>: Due Monday, September 16, 11:59pm</p>
</li>
<li>
<p><strong>Final Submission</strong>: Due <em>Saturday</em>, September 21, 11:59pm <br> (due on Saturday to accomodate the <a href="https://registrar.mit.edu/calendar">student holiday on Friday</a>)</p>
</li>
</ul>
<p>See the “Deliverables” section at the end of this document for more information on what you’ll be turning in.</p>
<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab2">lab repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab2.git</span>
</span></pre>
<h3>Changes to Starter Code vs Lab 1</h3>
<p>For Lab 2, you should be aware that we’ve made a few changes to the reference implementation of the Mandelbrot algorithm in the starter code, relative to what was in the starter code for Lab 1.</p>
<p>First of all, the line <code>float y = w - x2 - y2 + cy;</code> is now <code>float y = w - (x2 + y2) + cy;</code>. This may seem like a trivial change, but it speeds up the algorithm by allowing the compiler to reuse the expression <code>x2 + y2</code> computed in the loop condition, and also slightly alters the algorithm’s output due to the (maddening) non-associativity of floating-point math.</p>
<p>Additionally, we’ve adjusted the parameters of the Mandelbrot image we’re generating to be more ambitious. The image is now:</p>
<ul>
<li>
<p><strong>Higher resolution.</strong></p>
</li>
<li>
<p><strong>Higher quality.</strong> (More iterations per pixel)</p>
</li>
<li>
<p><strong>Zoomed in on a more interesting part of the Mandelbrot set.</strong> <br> (Check out the images in <code>./telerun-out</code> to see what it looks like!)</p>
</li>
</ul>
<p>To control what part of the Mandelbrot set we’re zoomed in to, we’ve added three new constant definitions in the starter code: <code>window_zoom</code>, <code>window_x</code>, and <code>window_y</code>. If you copy any code you wrote for Lab 1 for use in this lab, you’ll need to adjust it to refer to these new constants instead of using the hard-coded window parameters we used in Lab 1.</p>
<h2>Introduction</h2>
<h3>Goals for This Lab</h3>
<p>In <a href="/fall24/labs/lab1">Lab 1</a>, we looked at ways of using <em>vector parallelism</em> to accelerate an algorithm for rendering the Mandelbrot fractal. We also promised that in the following week, we would look at ways to add more levels of parallelism. The time has come to start exploring those additional levels of parallelism!</p>
<p>In this lab, we’ll be starting from our vector-parallel Mandelbrot implementation and adding three new kinds of parallelism:</p>
<ol>
<li>
<p><strong>Instruction-Level Parallelism:</strong> parallelizing execution of different instructions within a single instruction stream.</p>
</li>
<li>
<p><strong>Multi-Core Parallelism:</strong> parallelizing across multiple physical cores, each running its own instruction stream.</p>
</li>
<li>
<p><strong>Multi-Threaded Parallelism:</strong> parallelizing execution of multiple instruction streams within a single physical core.</p>
</li>
</ol>
<p>Just like we did in Lab 1, throughout this lab we’ll be focusing on the <strong>correspondences</strong> between the different levels of parallelism which exist in CPUs and GPUs. As we’ll see, the analogies between CPU and GPU hardware remain quite close even as we scale up our programs to start exploiting instruction-level, multi-core, and multi-threaded parallelism.</p>
<p>In this lab, we’ll also finally start to see our GPU programs overtaking our CPU programs in performance, as we write programs which are able to use the GPU to its full potential.</p>
<h3>A Note on Terminology: What is a “Thread?” (And What is a “Warp?”)</h3>
<p>Before we get into the details of the lab, we want to clarify some terminology which can become very confusing when talking about GPUs.</p>

<p>As you might have guessed from the use of the variable <code>threadIdx.x</code> in Lab 1, NVIDIA uses the word “thread” to refer to <em>a single instance of the program defined by your kernel</em>. But because the GPU executes all instructions in a SIMD fashion across 32 instances of your program at a time, what NVIDIA refers to as a “thread” is much closer to what we’d refer to as a <strong>vector lane</strong> on a CPU.</p>
<p>By contrast, NVIDIA uses the term “<a href="https://en.wikipedia.org/wiki/Warp_and_weft">warp</a>” to refer to what we’d call a “thread” on a CPU: a single stream of instructions which logically executes sequentially, and that stream’s associated register state. Note that because all instructions executed on an NVIDIA GPU are executed in 32-wide SIMD fashion, a “warp” consists of 32 of the-things-NVIDIA-calls-threads.</p>
<p>For the purposes of this course, we will try to <strong>always</strong> explicitly use the term “<strong>CUDA thread</strong>” when referring to the things NVIDIA calls threads, and we encourage you to do the same during live lab, in write-ups, on Piazza, etc.</p>
<p>When we talk about “threads” generically, as in the phrase “multi-threading,” you should understand that in the context of GPUs we’re referring to the same thing that NVIDIA calls “warps.”</p>
<p>We can summarize the situation in the following table of correspondences:</p>
<div class="table-container"><table><thead><tr><th>GPU Concept</th><th>CPU Concept</th></tr></thead><tbody>
<tr><td>CUDA Thread</td><td>Vector Lane</td></tr>
<tr><td>Warp</td><td>Thread</td></tr>
</tbody></table>
</div>
<h3>Background: A Closer Look at the Hardware</h3>
<p>Before we start extending our code, it will be useful to have a clear mental picture of what hardware resources are available on the machines we’re programming.</p>

<h4>The CPU</h4>
<p>The CPU we’re using for this lab is an <a href="https://www.techpowerup.com/cpu-specs/ryzen-7-7700.c2960">AMD Ryzen 7 7700</a>. This CPU is clocked at 3.8 GHz and consists of <strong>8 <em>cores</em></strong>, each of which can execute its own independent stream of scalar and vector instructions. The scalar and vector programs we wrote in Lab 1 made use of only a <strong>single</strong> one of these cores, and ignored the other 7.</p>
<p>Each core has its own <strong>register file</strong> for storing thread state, its own <strong>control logic</strong>, and its own <strong>functional units</strong> like ALUs and FPUs. Although it’s not too relevant for the compute-dominated workload we’re studying in this lab, each core also has its own <strong>L1 cache</strong>.</p>
<p>Each core on this CPU supports <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading">simultaneous multi-threading (SMT)</a> with <strong>at most 2 concurrent <em>threads</em></strong> of execution, with each thread having its own program counter and register state. When different instructions from these threads require the use of different hardware resources, they are able to execute in parallel; when they contend for the same shared hardware resources, they are forced to execute in sequence.</p>
<p>We show a schematic view of the CPU below:</p>
<p><img src="images/cpu_schematic.svg" alt="CPU schematic" /></p>
<h4>The GPU</h4>
<p>The GPU we’re using for this lab is an <a href="https://www.techpowerup.com/gpu-specs/rtx-a4000.c3756">NVIDIA RTX A4000</a>, clocked at 1.56 GHz. Unlike the CPU, which consists of a flat array of cores, the processing elements on our GPU are organized into a two-level hierarchy. In NVIDIA’s terminology:</p>
<ol>
<li>
<p>The GPU consists of <strong>48 <em>streaming multiprocessors</em> (SMs)</strong>.</p>
</li>
<li>
<p>Each SM consists of <strong>4 <em>warp schedulers</em></strong>. <sup class="footnote-reference"><a href="#1">1</a></sup></p>
</li>
</ol>
<p>For our purposes, we can regard each individual warp scheduler as <strong>analogous to a CPU core</strong>. Just like a CPU core, each warp scheduler can execute its own independent stream of vector instructions, and has its own register file, control logic, and functional units. In this lab, we’ll use the word “core” generically to refer to both CPU cores and to warp schedulers.</p>
<p>If a warp scheduler is analogous to a CPU core, you may be wondering about the significance of the “SM” level of the hierarchy – why not just think of the GPU as a flat array of 192 warp schedulers? The answer lies in the memory system: the <strong>L1 cache</strong>, as well as some other memory resources, are <strong>shared at the level of an SM</strong>. The fact that the L1 cache is shared won’t be directly relevant for this lab, but it may be helpful for understanding why the concept of an “SM” exists at all.</p>
<p>Similar to the cores on our CPU, each warp scheduler can manage multiple concurrent warps at a time. Specifically, each warp scheduler on our GPU supports <a href="https://en.wikipedia.org/wiki/Temporal_multithreading">fine-grained temporal multi-threading</a> between <strong>at most 12 concurrent warps</strong>. The warp scheduler can issue an instruction for at most one warp per cycle, and multi-cycle instructions from different warps can execute in parallel with each other as long as they don’t contend for the same shared hardware resources.</p>
<p>Just like on the CPU, the scalar and vector GPU programs we wrote in Lab 1 made use of only a <strong>single</strong> warp scheduler, and ignored the other 191 (!) warp schedulers on the machine.</p>
<p>We show a schematic view of the GPU below:</p>
<!-- * **Clock frequency:** 1.6 GHz
* **Number of warp schedulers:** 192
* **Max concurrent warps per warp scheduler:** 12 -->

<p><img src="images/gpu_schematic.svg" alt="GPU schematic" /></p>
<h2>Part 1: Instruction-Level Parallelism</h2>
<p>In Lab 1, the programs we wrote each made use of only a <strong>single thread</strong>, running on a <strong>single core</strong>. Later in this lab, we’ll be scaling up our programs to make use of all the cores on our CPU and GPU. But first, we’ll look at a way that we can exploit <strong>more parallelism</strong> in our vector Mandelbrot program <strong>using just the single thread that we already have.</strong></p>
<p>To find more opportunities for parallelism within our single thread, we’ll make use of what is known as <em><a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">instruction-level parallelism (ILP)</a></em>. Instruction-level parallelism refers to cases where a processor is able to exploit <strong>independence</strong> between instructions within one instruction stream in order to run them in parallel on different hardware resources.</p>
<p>The exact microarchitectural details of when and how the processor can exploit ILP differ between the CPU and GPU <sup class="footnote-reference"><a href="#2">2</a></sup>, but the implications for software performance engineering are similar on both platforms. In general, sections of code with <strong>sequential dependencies</strong> like the following will not be able to exploit ILP:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> No ILP:
</span><span class="highlight-storage highlight-type highlight-c">float</span> a0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-comment highlight-block highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">/*</span> ... <span class="highlight-punctuation highlight-definition highlight-comment highlight-c">*/</span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a0 <span class="highlight-keyword highlight-operator highlight-c">*</span> a0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a1 <span class="highlight-keyword highlight-operator highlight-c">*</span> a1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> a3 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a2 <span class="highlight-keyword highlight-operator highlight-c">*</span> a2<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>whereas code containing <strong>independent instructions</strong> will provide the processor with opportunities to exploit ILP:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-comment highlight-line highlight-double-slash highlight-c"><span class="highlight-punctuation highlight-definition highlight-comment highlight-c">//</span> Opportunities for ILP:
</span><span class="highlight-storage highlight-type highlight-c">float</span> b0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c0 <span class="highlight-keyword highlight-operator highlight-c">*</span> c0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c1 <span class="highlight-keyword highlight-operator highlight-c">*</span> c1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c2 <span class="highlight-keyword highlight-operator highlight-c">*</span> c2<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b3 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c3 <span class="highlight-keyword highlight-operator highlight-c">*</span> c3<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>Depending on the latency and throughput with which the processor can execute a given type of instruction, running multiple instructions in parallel is often necessary in order to drive the hardware at its peak throughput. For example, on our A4000 GPU, basic 32-bit floating point arithmetic instructions like multiplies and adds…</p>
<ol>
<li>
<p>…execute with a <strong>latency</strong> of <strong>4 cycles per instruction</strong>… <sup class="footnote-reference"><a href="#latency-source">3</a></sup></p>
</li>
<li>
<p>…but can execute with a <strong>peak throughput</strong> of <strong>1 instruction per cycle per warp scheduler</strong>. <sup class="footnote-reference"><a href="#throughput-source">4</a></sup></p>
</li>
</ol>
<p>This means that to execute 32-bit floating point multiplies and adds at peak throughput, you need to have <strong>at least 4</strong> independent multiply or add instructions in flight <strong>simultaneously</strong> at any given time. You can think of e.g. the floating point multiplier as a pipeline with 4 stages, which can only run at its peak throughput if you’re able to keep each of those 4 pipeline stages busy processing a different instruction on each cycle.</p>
<p>Revisiting our Mandelbrot program, we can analyze its inner loop to see what kind of opportunities for ILP it exposes to the processor:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-keyword highlight-control highlight-c++">while</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span>x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y2 <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;=</span> <span class="highlight-constant highlight-numeric highlight-c++">4.0f</span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">&amp;&amp;</span> iters <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> max_iters<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> x <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">-</span> y2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> cx<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> y <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> w <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">-</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span>x2 <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y2<span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> cy<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    x2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x <span class="highlight-keyword highlight-operator highlight-c">*</span> x<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    y2 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> y <span class="highlight-keyword highlight-operator highlight-c">*</span> y<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-storage highlight-type highlight-c">float</span> z <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> x <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span> y<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    w <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> z <span class="highlight-keyword highlight-operator highlight-c">*</span> z<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
    <span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span>iters<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
</span></pre>
<p>We can see that our Mandelbrot program <em>already</em> provides the processor with some opportunities to exploit ILP: for example, the instructions <code>x2 = x * x;</code>, <code>y2 = y * y;</code>, and <code>z = x + y;</code> are independent. On the other hand, there are also some sequential bottlenecks: for example, the instruction <code>w = z * z;</code> depends directly on the previous instruction <code>z = x + y;</code>.</p>
<p>Using the GPU version of our program as an example, we can visualize the dependency relationships between the different <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIAruiakl9ZATwDKjdAGFUtEywYgATKUcAZPAZMADl3ACNMYhAAZgBWUgAHVAVCOwYXNw9vROTUgQCg0JYIqLjLTGtbASECJmICDPdPHytMGzSauoICkPDImPiFWvrGrJbh7sDe4v64gEpLVBNiZHYOAFIvaMDkNywAanXop2QWJgIEI%2Bx1jQBBTe2GXbNMQ%2BPkIfxBK5v7u7QGENMKoEsR9lRaKhzvsAO6BdCoGEAfQAXqhUCw3gARfZcAB0Gn2AHpcRoyRoCVQjgAhX4AoEgsEQqEEWHwxFI1TY/YAWgpAHZJNEAGxCgAcYv5vP2FNi%2BwAVGyGAjkWiMTS6QIGaDwZDoXDlRyAJ7cilcaJcMVeWIATg0kmlsoVSpVqPRLA1d1%2BRJJTmUcn2IjEdX2AFlDFhaGFiKhWUpWcBGJFzmk8d6SdSmEp0PsBPsLq9Nl5UAlbCw8CjMDnMApRAlXmXXmJgCRCAgPV4vPtAun9ggCAQEgoQD7GHi4QBrPD1/BMPEkYBEyd4InKSEDwLAJHN1sXFgKJH8YhIgtI8PKirR2NI%2BO/JFI4CQsJie/7ABuqDwObOF6jMYID4JCYN6iAYxAQCYgQENEXgnt2LBbiklakPskGCDBcFnKoSKEJECgoWh0Gway8pLAQcyHPytJ3PstHgiQ%2BwQVBGGsngpo0t2bxOPBiEVpgHGbLSXjUngFHrFRvx0VJ9Fgkx6HEfs2jsdE1KKVxPE3nxAnCYJ2hiRJNHSUZzLQsgXJHDi1B6gQEB6cSuoshAeAIZplZzBRioGq6aqYoJLocqonq3EZxnWfsyAmhZjEmTZon2TFTkuUhmDuc6Xkcj5hzCf5yJGkFkkhVJMX7KoXZRRSGhUipBWFbRxVGmV0Q4hVVXUcFtV1WFMKmpSQUdbRhEsd2BB4cpbX9bCCB0K8EClVlqkNVxUWSJSWXCpswrDaNRzcVhOEjcQCj6eNE10cV5lNSVXY8vsi1%2BWZfWnZ1LK3dy3U3bNZXZQ1YnZRFj1PXNUVcoqgXVYZT13ZdJqKnl4PtU9xUoty5nfQDp3dVFyOKii6MTYJgm4YdeMheJWK/DVtU%2BvsADqxC4fsxA1iYtAEGmEO1WR6yxCJzrObxlbzYp3M4lFRMKCTtFk5T0tevyWIcAstCcLEvCeNwvCoJwThyFiwVOAT2UKEsKyFlsPCkAQmiK6QEBIGgLAJNNZAUBADtO/QUQMG%2ByDIFwXiSCYXB8HQB3DhAYTW6QYSBHURqcBbMfMMQRoAPJhNobRWxrpAO2wgipwwtDxxwWikFgYQmMAThiLQw451gP6PmsZf4Ez7RvjWUfAm0JgjQnvBQRUUe0Hg0Zxy4WBRwQ9MsAPpCd8QYTJJgWKYE3o9GNbCwQkwwAKAAangmAwqn9bqxb/CCEG4hSDIgiKCo6ilzoegGFvpjmPoY/DpACwllUQEnAeSp2iLyHkJglCHizABM4FxeSJieBgJsKxLhNXdn3TASIxTClzsgiyCgWDYOFJrRe9MsC/wgAsVo7R7AQEcKMTwwd/BTCKCUPQSQUiAMYRw3IgCehsP6MHGhgDOgjFcE0PQIiOgTAEX0KIwiJg8MUV0ORMwFHUJNqsCQSsVZqyjlrDg%2BxggHycNxf2eIVpcEYrgQgDEHhcDmLwbOWh3KkAnCAWIGh9CcEkPol%2BpBDG8GHN4lxNs4CwCQFgN8eBVjkEoO7Z2wRWBrBMWY3EXhLF4gttE2J7BlAABUAAavh8BEHIXoK%2BwhQLsDvlUx%2Bago66GERULOaQHDKmUb4ZUaj2HB04XkdIEisj9L4WkXpQjyiVBkV0Lp0jqiyNYfIqRSjhlMMsIswoyzHGWyZpgL8IANC6I4KrUg6sy6GLXjE1Y%2Bwj4n0iMY0x5jMlWJsWU%2Bx5sUIuEds7LKFonGW23gsBAmAmBYCiFQ9xnjvHKw4H4s5BjODBMOYCl%2BbjYVeH8RcpFqLXELEXikewkggA%3D">PTX assembly</a> instructions in the inner loop as follows:</p>
<p><img src="images/ilp_graph.svg" alt="Instruction dependency graph" /></p>
<p>(If that’s hard to read, you can view the full-size image <a href="images/ilp_graph.svg">here</a>.)</p>
<p>Even though this sequence of instructions contains some amount ILP, that ILP is not sufficient to fully utilize the hardware of one of our GPU’s warp schedulers: in particular, there is no point in the inner loop where the processor has the opportunity to run 4 independent floating-point arithmetic instructions simultaneously. Similar considerations apply to our CPU version.</p>
<h3>Implementation: Increasing ILP</h3>
<p>At first, it might seem like there isn’t much we can do to increase the amount of ILP in our program. It looks like some of the instructions in our algorithm just fundamentally need the results of earlier instructions to be available before they can execute.</p>
<p>If we only cared about computing the value of a single pixel, or a single vector of pixels, it would indeed be difficult to increase the amount of ILP in our program. But we don’t want to compute just a single vector of pixels – we want to compute <strong>thousands</strong> of vectors of pixels! Even if the instructions required to compute each individual vector of pixels are inherently sequential, the instructions required to compute <em>multiple different vectors</em> of pixels are independent of each other. We can exploit this observation to significantly increase ILP.</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_ilp</code> using an algorithm employing both vectorization (from Lab 1) and techniques to increase ILP. You can increase ILP by reorganizing your computation so that <strong>each</strong> inner loop iteration processes <strong>multiple independent vectors’ worth of pixels</strong> at a time.</p>
</blockquote>
<blockquote>
<p><strong>Deliverable:</strong> Do the same for the GPU, by editing the file <code>mandelbrot_gpu_2.cu</code> to implement the functions <code>mandelbrot_gpu_vector_ilp</code> and <code>launch_mandelbrot_gpu_vector_ilp</code>. Keep the launch configuration of your kernel at <code>&lt;&lt;&lt;1, 32&gt;&gt;&gt;</code>, like we did in Lab 1; this is directly analogous to how we’re running a single thread of vector instructions on the CPU.</p>
</blockquote>
<p>Here are some things you might want to consider in your CPU and GPU implementations:</p>
<ol>
<li>
<p>What should happen to the <strong>state variables</strong> in your program if you’re processing multiple vectors’ worth of pixels at a time? For example, how should you deal with the variables <code>x2</code> and <code>y2</code>?</p>
</li>
<li>
<p>How should you handle <strong>control flow</strong>? How do you deal with differences between the number of inner loop iterations required to compute different vectors of pixels?</p>
</li>
<li>
<p>Do you think certain <strong>control flow patterns</strong> might impose <strong>different costs and benefits</strong> on the <strong>CPU vs the GPU</strong>? What are some possible control flow strategies you might try on each platform?</p>
</li>
<li>
<p><strong>How many</strong> different vectors of pixels should you work on at a time? Can you set up a system that lets you easily test the effects of trying different values for that parameter?</p>
</li>
<li>
<p><strong>Where in the image</strong> do you want to take your multiple vectors of pixels from? Do you want to process one long row at a time? Or 2D rectangular tiles? Something more complicated?</p>
</li>
</ol>
<p>Additionally, when you write your code, you may find it helpful to know two non-obvious facts about C/C++/CUDA compilers:</p>
<ol>
<li>
<p>Modern compilers are excellent at <strong>automatically reordering independent instructions</strong> to increase ILP, as long as the instructions are all within one block without control flow in between. For example, if you write: <br></p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-storage highlight-type highlight-c">float</span> b0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a0 <span class="highlight-keyword highlight-operator highlight-c">*</span> a0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> c0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> b0 <span class="highlight-keyword highlight-operator highlight-c">*</span> b0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> d0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c0 <span class="highlight-keyword highlight-operator highlight-c">*</span> c0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>

<span class="highlight-storage highlight-type highlight-c">float</span> b1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a1 <span class="highlight-keyword highlight-operator highlight-c">*</span> a1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> c1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> b1 <span class="highlight-keyword highlight-operator highlight-c">*</span> b1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> d1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c1 <span class="highlight-keyword highlight-operator highlight-c">*</span> c1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>you can trust the compiler to reliably transform it to something like:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-storage highlight-type highlight-c">float</span> b0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a0 <span class="highlight-keyword highlight-operator highlight-c">*</span> a0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> b1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> a1 <span class="highlight-keyword highlight-operator highlight-c">*</span> a1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>

<span class="highlight-storage highlight-type highlight-c">float</span> c0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> b0 <span class="highlight-keyword highlight-operator highlight-c">*</span> b0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> c1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> b1 <span class="highlight-keyword highlight-operator highlight-c">*</span> b1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>

<span class="highlight-storage highlight-type highlight-c">float</span> d0 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c0 <span class="highlight-keyword highlight-operator highlight-c">*</span> c0<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-storage highlight-type highlight-c">float</span> d1 <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> c1 <span class="highlight-keyword highlight-operator highlight-c">*</span> c1<span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>This means you don’t need to worry too much about the fine-grained order in which your code appears to execute independent instructions. You can simply throw all your instructions into one big “soup” and let the compiler sort it out on the other end.</p>
</li>
<li>
<p>The compiler supports an annotation called <strong><code>#pragma unroll</code></strong> (<a href="https://releases.llvm.org/4.0.0/tools/clang/docs/AttributeReference.html#pragma-unroll-pragma-nounroll">link</a>) which can be used to replace an <code>n</code>-iteration <code>for</code> loop with <code>n</code> copies of its body. For example, if you write: <br></p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-meta highlight-preprocessor highlight-c++"><span class="highlight-keyword highlight-control highlight-import highlight-c++">#pragma</span> unroll
</span><span class="highlight-keyword highlight-control highlight-c++">for</span> <span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span><span class="highlight-support highlight-type highlight-stdint highlight-c">uint32_t</span> i <span class="highlight-keyword highlight-operator highlight-assignment highlight-c">=</span> <span class="highlight-constant highlight-numeric highlight-c++">0</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> i <span class="highlight-keyword highlight-operator highlight-comparison highlight-c">&lt;</span> <span class="highlight-constant highlight-numeric highlight-c++">4</span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span> i<span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-keyword highlight-operator highlight-arithmetic highlight-c">+</span><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span> <span class="highlight-meta highlight-block highlight-c++"><span class="highlight-punctuation highlight-section highlight-block highlight-begin highlight-c++">{</span>
    <span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++">i</span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-punctuation highlight-section highlight-block highlight-end highlight-c++">}</span></span>
</span></pre>
<p>that will compile to code equivalent to:</p>
<pre><span class="highlight-source highlight-c++"><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">0</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">1</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">2</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
<span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-variable highlight-function highlight-c++">f</span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-begin highlight-c++">(</span></span></span><span class="highlight-meta highlight-function-call highlight-c++"><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-constant highlight-numeric highlight-c++">3</span></span><span class="highlight-meta highlight-group highlight-c++"><span class="highlight-punctuation highlight-section highlight-group highlight-end highlight-c++">)</span></span></span><span class="highlight-punctuation highlight-terminator highlight-c++">;</span>
</span></pre>
<p>The compiler will also often perform this kind of unrolling automatically when it guesses that doing so will improve performance, but using <code>#pragma unroll</code> manually provides a stronger guarantee.</p>
</li>
</ol>
<p>After you’ve written your vector + ILP Mandelbrot implementations for the CPU and GPU, you can include your answer to the following in your final write-up:</p>
<blockquote>
<p><strong>Question 1 for final write-up:</strong> What speedup do you see from increasing the amount of ILP in your CPU and GPU Mandelbrot implementations? What strategy did you use for partitioning the image into groups of vectors, and why did you choose it? How did you deal with managing control flow and state on the CPU and GPU? How many different vectors did you choose to process at once in your CPU and GPU implementations, and why? What seem to be the limiting factors on how far you can scale ILP?</p>
</blockquote>
<h2>Part 2: Multi-Core Parallelism</h2>
<p>So far in Lab 1 and Lab 2, we’ve seen how to push the limits of performance for a <em>single thread</em> running on a <em>single core</em> on both CPUs and GPUs, by exploiting a combination of vector parallelism and instruction-level parallelism. Now it’s finally time to start scaling our programs in the other direction, to run across <strong>multiple cores</strong>.</p>
<p>In this section, we’ll look at how multi-core parallelism is exposed at the software level on CPUs and on GPUs, and implement <strong>corresponding</strong> Mandelbrot algorithms for both platforms that run <strong>one thread on each core</strong> of their respective machines.</p>
<p>Note that, for the sake of simplicity, we’ll be <strong>setting aside our ILP-optimized implementations</strong> from the previous section for now, and working from our <strong>original vector-parallel</strong> implementations as a starting point instead. We’ll pick the ILP-optimized implementations back up again in Part 4.</p>
<h3>Implementation: CPU Multi-Core</h3>
<p>On the CPU, we’ll be accessing multi-core parallelism via the <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html">POSIX thread (“pthread”) API</a>, which you may have previously encountered in another computer systems or performance engineering class. Our goal is to use the pthread API to:</p>
<ol>
<li>
<p>Spawn <strong>8 child CPU threads</strong>, one for each core of the machine.</p>
</li>
<li>
<p>Run a <strong>vector-parallel</strong> computation on each of those 8 threads to compute the Mandelbrot fractal.</p>
</li>
<li>
<p><strong>Synchronize</strong> on completion of the 8 threads, so that we don’t return until we’re sure that all 8 threads have finished.</p>
</li>
</ol>
<p>This leads to the first deliverable of Part 2:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore</code> as described.</p>
</blockquote>
<p>For your implementation, you will likely want to look at the functions <a href="https://man7.org/linux/man-pages/man3/pthread_create.3.html"><code>pthread_create(...)</code></a> and <a href="https://man7.org/linux/man-pages/man3/pthread_join.3.html"><code>pthread_join(...)</code></a>.</p>
<p>You may want to think about different ways that you could <strong>partition work</strong> between the threads you’re spawning. How do you plan to decide what subset of the image each thread will be responsible for?</p>
<blockquote>
<p><strong>Question 2 for final write-up:</strong> What speedup over the single-core vector-parallel CPU implementation do you see from parallelizing over 8 cores? How do you think the work partitioning strategy might affect the end-to-end performance of the program?</p>
</blockquote>
<h3>Implementation: GPU Multi-Core</h3>
<p>By direct analogy to running one (vector-parallel) thread on each of the 8 cores of the CPU, on the GPU our goal for this section is to run one <strong>warp</strong> on each of the GPU’s <strong>192 warp schedulers</strong>.</p>
<p>Conceptually, what we want to do is to launch our kernel with <code>192 * 32 = 6144</code> CUDA threads, and have each group of 32 of those CUDA threads run on a separate warp scheduler. However, we can’t just launch our kernel with launch parameters</p>
<pre><code>&lt;&lt;&lt;1, 6144&gt;&gt;&gt;
</code></pre>
<p>because CUDA uses a <strong>two-level hierarchy of software parallelism</strong> which corresponds roughly to the two-level  hierarchy of SMs and warp schedulers which is present in the hardware. Specifically:</p>
<ol>
<li>
<p><strong>CUDA threads are organized into groups called “blocks.”</strong> <sup class="footnote-reference"><a href="#cta">5</a></sup> When you launch a kernel, you specify both the number of blocks to launch, and the number of CUDA threads to launch per block. These are the two numbers in the <code>&lt;&lt;&lt;..., ...&gt;&gt;&gt;</code> kernel launch angle bracket notation.</p>
</li>
<li>
<p><strong>All the warps in a given block are guaranteed to run on the same SM.</strong> It is possible to have multiple blocks run on the same SM simultaneously, but not vice-versa.</p>
</li>
</ol>
<p>Like SMs themselves, blocks exists mostly because of the memory system; blocks define the level at which warps share certain memory resources on the software side, in the same way that SMs define the level at which warp schedulers share certain memory resources on the hardware side. In this lab, we’re not concerned with the memory system, but we hope providing this context helps provide some clarity on why blocks exist in the first place.</p>
<p>The existence of blocks means that if you want to use more than one SM, you need to launch more than one block. In our case, we want to use all 48 SMs on the GPU, and all 4 warp schedulers on each SM. One reasonable way to achieve this is to launch a kernel with launch configuration <code>&lt;&lt;&lt;48, 4 * 32&gt;&gt;&gt;</code>, which will assign one block to each SM, and then assign the 4 warps inside that block to the SM’s 4 warp schedulers.</p>
<p>This gives us almost all the pieces we need to scale up our Mandelbrot kernel to use every warp scheduler on the GPU. There are just two more things you strictly need to know:</p>
<ol>
<li>
<p>The value of <code>threadIdx.x</code> is unique per CUDA thread within a block, and resets across blocks. That means that in any given block, warp <code>0</code> will have <code>threadIdx.x</code> values <code>0, ..., 31</code>, warp <code>1</code> will have <code>threadIdx.x</code> values <code>32, ..., 63</code>, and so on. <sup class="footnote-reference"><a href="#3D">6</a></sup></p>
</li>
<li>
<p>You can access the index of the current block using the magic variable <code>blockIdx.x</code>.</p>
</li>
</ol>
<p>Finally, you may find it helpful to know that you can access the total number of blocks the kernel was launched with using the variable <code>grimDim.x</code>, and access the number of CUDA threads per block with the variable <code>blockDim.x</code>.</p>
<p>We’re now ready to write the multi-core vector-parallel GPU Mandelbrot implementation:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_gpu_vector_multicore</code> and <code>launch_mandelbrot_gpu_vector_multicore</code> to run exactly one warp on each warp scheduler of the GPU.</p>
</blockquote>
<p>As in the CPU case, you may want to consider different partitioning strategies for deciding which subset of the image each warp will be responsible for.</p>
<blockquote>
<p><strong>Question 3 for final write-up:</strong> What speedup over the single-warp vector-parallel GPU implementation do you see from parallelizing over 192 warp schedulers? How does the absolute run time of the GPU multi-core version compare to the CPU multi-core version? How did you approach designing the work partitioning strategy?</p>
</blockquote>
<blockquote>
<p><strong>Question 4 for final write-up:</strong> Try adapting the kernel to use a launch configuration of <code>&lt;&lt;&lt;96, 2 * 32&gt;&gt;&gt;</code>. Will this still assign exactly one warp to every warp scheduler on the machine? How about <code>&lt;&lt;&lt;24, 8 * 32&gt;&gt;&gt;</code>? How do the run times of all of these configurations compare to each other?</p>
</blockquote>
<h2>Part 3: Multi-Threaded Parallelism</h2>
<p>Even though we’re now running code on every core of both our CPU and GPU, there is still one more level of parallelism left to exploit. As we saw at the beginning of this lab, both the cores on our CPU and the warp schedulers on our GPU support <strong>multi-threading</strong>, allowing for concurrent execution of instructions from multiple threads on a single core. By spawning more threads than we have cores, we can start to take advantage of the additional opportunities for parallelism exposed by multi-threading.</p>
<h3>Implementation: CPU Multi-Threaded</h3>
<p>We can adapt our multi-core CPU implementation to use multiple threads per core simply by increasing the number of threads:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore_multithread</code> to spawn <code>&gt; 1</code> thread per CPU core. As before, every thread in this new implementation should continue to exploit vector parallelism. You can choose the exact number of threads spawned as you see fit.</p>
</blockquote>
<blockquote>
<p><strong>Question 5 for final write-up:</strong> How much are you able to speed up your implementation by introducing multi-threading per-core? What seems to be the optimal number of threads to spawn? What factors do you think might contribute to determining the optimal number of threads?</p>
</blockquote>
<h3>Implementation: GPU Multi-Threaded</h3>
<p>As in the CPU case, we can introduce multi-threading per-core into our multi-core GPU implementation just by launching more warps than we have warp schedulers across the machine.</p>
<p>Before we do that, however, it may be instructive to look specifically at how performance varies as a function of number of warps <strong>if we restrict the kernel’s execution to a single block on a single SM</strong>, which allows us to get a particularly clean view of the effect of multi-threading without needing to worry about effects related to block-level scheduling and workload assignment:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_cpu_vector_multicore_multithread_single_sm</code> and <code>launch_cpu_vector_multicore_multithread_single_sm</code> to run as many warps as you want (up to the hardware limit of <code>48</code> warps per block), but all inside just a <strong>single block</strong>.</p>
</blockquote>
<blockquote>
<p><strong>Question 6 for final write-up:</strong> In the <code>mandelbrot_cpu_vector_multicore_multithread_single_sm</code> kernel, how does run time vary as a function of the number of warps, beyond the point where there is one warp to populate each of the 4 warp schedulers on the SM? Does it keep improving all the way up to the hard limit of <code>48</code> warps per block? If so, by how much? What factors do you think might be contributing to what you observe?</p>
</blockquote>
<p>Returning to a more realistic setting, analogous to the multi-threaded CPU implementation, we can finally write our full-scale multi-threaded GPU implementation:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, implement the functions <code>mandelbrot_gpu_vector_multicore_multithread_full</code> and <code>launch_mandelbrot_gpu_vector_multicore_multithread_full</code> to run as many warps as you want across as many blocks as you want.</p>
</blockquote>
<blockquote>
<p><strong>Question 7 for final write-up:</strong> As in the CPU case: How much are you able to speed up your GPU implementation by introducing multi-threading per-warp-scheduler? What seems to be the optimal number of warps to spawn? What factors do you think might contribute to determining the optimal number of warps?</p>
</blockquote>
<h2>Part 4: Putting It All Together</h2>
<p>Now that we have seen how to saturate both the CPU and GPU’s capacity for vector parallelism, multi-core parallelism, and multi-threaded parallelism, there is just one remaining thing left to try, which is to <strong>add back in the ILP optimizations</strong> we introduced in Part 1 and discarded in Part 2:</p>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_cpu_2.cpp</code>, implement the function <code>mandelbrot_cpu_vector_multicore_multithread_ilp</code> by combining the techniques you used to increase ILP in <code>mandelbrot_cpu_vector_ilp</code> with the techniques you used to achieve multi-core and multi-threaded parallelism in <code>mandelbrot_cpu_vector_multicore_multithread</code>.</p>
</blockquote>
<blockquote>
<p><strong>Deliverable:</strong> In the file <code>mandelbrot_gpu_2.cu</code>, do the same for your GPU implementations in the functions <code>mandelbrot_gpu_vector_multicore_multithread_full_ilp</code> and <code>launch_mandelbrot_gpu_vector_multicore_multithread_full_ilp</code>.</p>
</blockquote>
<blockquote>
<p><strong>Question 8 for final write-up:</strong> In your CPU and GPU implementations, how much speedup, if any, were you able to achieve by adding your ILP optimizations back in on top of your multi-core, multi-threaded algorithms? How does the speedup from increasing ILP in this setting compare to the speedup from increasing ILP in the single-threaded, single-core setting? What seems to be the optimal number of threads in this setting on CPU and GPU, and what is the optimal number of independent vectors of pixels to process at once in the inner loop? What factors do you think might be contributing to what you observe?</p>
</blockquote>
<h2>Deliverables</h2>
<h3>Checkpoint (Due Monday, September 16, 11:59pm)</h3>
<p>As with last week’s lab, you do not need to submit any code for the initial checkpoint, but you should have made some effort to get started on the lab. Ideally, we’d like to see you try to make some progress on Part 1 and Part 2 – if you’re stuck on Part 1 before the live lab, it’s fine to set it aside and move on to trying Part 2.</p>
<p>On the Gradescope assignment “Lab 2 Checkpoint,” (<a href="https://www.gradescope.com/courses/849967/assignments/4955974/">Link</a>) submit brief answers to the two prompts checking in about how you’re doing with the lab.</p>
<h3>Final Submission (Due <em>Saturday</em>, September 21, 11:59pm)</h3>
<p>On the Gradescope assignment “Lab 2 Final,” submit your <strong>completed code</strong> for <code>mandelbrot_cpu_2.cpp</code> and <code>mandelbrot_gpu_2.cpp</code>, as well as a PDF writeup containing your answers to Questions 1 - 8 above, answering each sub-question with roughly a sentence or two.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Some NVIDIA documentation refers to these four elements of the SM as “partitions,” and uses the term “warp scheduler” to refer to just the control logic of a partition. Our use of the term “warp scheduler” to refer to the whole unit is therefore a kind of <a href="https://en.wikipedia.org/wiki/Synecdoche">synecdoche</a>. Other authors refer to these elements of the SM as “<a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">quadrants</a>.”</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>In particular, the cores on the CPU we’re using are <a href="https://en.wikipedia.org/wiki/Wide-issue">wide-issue</a> and <a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order</a>, whereas the cores on our GPU are single-issue and in-order.</p>
</div>
<div class="footnote-definition" id="latency-source"><sup class="footnote-definition-label">3</sup>
<p><a href="https://docs.nvidia.com/cuda/turing-tuning-guide/index.html#instruction-scheduling">Source</a> for the 4-cycle latency figure. The linked document was written for slightly older hardware than we’re using, but should still apply.</p>
</div>
<div class="footnote-definition" id="throughput-source"><sup class="footnote-definition-label">4</sup>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions">Source</a> for the 1 instruction per cycle throughput figure. In the linked table, the “compute capability” column corresponding to the specific GPU we’re using is “8.6.”</p>
</div>
<div class="footnote-definition" id="cta"><sup class="footnote-definition-label">5</sup>
<p>“Blocks” are sometimes also called “cooperative thread arrays” (CTAs), and NVIDIA’s documentation uses the two terms interchangeably.</p>
</div>
<div class="footnote-definition" id="3D"><sup class="footnote-definition-label">6</sup>
<p>We’re deliberately ignoring the <code>y</code> and <code>z</code> dimensions of CUDA’s <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy">3D parallelism</a> in this lab, because 3D parallelism is essentially just a thin layer of user-interface convenience over what is fundamentally a <em>linear</em> index space.</p>
</div>
</main><footer><p><a href="https://mit.edu">Massachusetts Institute of Technology</a> —
<a href="https://www.eecs.mit.edu">Department of Electrical Engineering and Computer Science</a></p>
</footer></body></html>